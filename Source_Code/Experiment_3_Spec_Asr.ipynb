{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847b2d33-2262-43af-84e2-894a2a272c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the zipfile module \n",
    "from zipfile import ZipFile \n",
    "  \n",
    "# loading the temp.zip and creating a zip object \n",
    "with ZipFile('train_split.zip', 'r') as zObject: \n",
    "  \n",
    "    # Extracting all the members of the zip  \n",
    "    # into a specific location. \n",
    "    zObject.extractall( \n",
    "        path=\"./\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334acd42-64fb-4e21-af0d-f05a2b757ecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy<2\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scipy 1.9.3 requires numpy<1.26.0,>=1.18.5, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.45.2)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
      "Requirement already satisfied: peft in ./.local/lib/python3.11/site-packages (0.15.2)\n",
      "Requirement already satisfied: jiwer in ./.local/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: soundfile in ./.local/lib/python3.11/site-packages (0.13.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: click>=8.1.8 in ./.local/lib/python3.11/site-packages (from jiwer) (8.2.1)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in ./.local/lib/python3.11/site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall \"numpy<2\"\n",
    "!pip install --upgrade pybind11>=2.12\n",
    "# Install required packages (omit tf-keras to prevent TF integration)\n",
    "!pip install transformers datasets accelerate peft jiwer soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5668c4d2-92f6-4adc-9812-ada527f98cba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 12:59:31.598001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751547571.611221     475 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751547571.615142     475 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751547571.626687     475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751547571.626707     475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751547571.626709     475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751547571.626710     475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73e81c75-b39b-4740-8b5f-9fcb9fb30449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.24.3\n",
      "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a204f02-9d63-4c8f-aa7e-431b67a128dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: soundfile in ./.local/lib/python3.11/site-packages (0.13.1)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.22.3 in ./.local/lib/python3.11/site-packages (from librosa) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.9.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (5.1.1)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.10.0)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (23.2)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.local/lib/python3.11/site-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (429 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m430.0/430.0 kB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.5.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.5/252.5 kB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: soxr, msgpack, llvmlite, lazy_loader, audioread, pooch, numba, librosa\n",
      "Successfully installed audioread-3.0.1 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.44.0 msgpack-1.1.1 numba-0.61.2 pooch-1.8.2 soxr-0.5.0.post1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9707fbb-6c44-4714-883e-044d0dcbf041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written manifest: hf_manifests/train_split.jsonl\n",
      "Written manifest: hf_manifests/test_split.jsonl\n"
     ]
    }
   ],
   "source": [
    "#DATA_ROOT = 'data'\n",
    "OUT_DIR = 'hf_manifests'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "for split in ['train_split', 'test_split']:\n",
    "    transcripts_dir = os.path.join(split, 'transcripts')\n",
    "    manifest_path = os.path.join(OUT_DIR, f'{split}.jsonl')\n",
    "    with open(manifest_path, 'w') as outf:\n",
    "        # load mapping\n",
    "        wav_scp = os.path.join(transcripts_dir, 'wav.scp')\n",
    "        segments_f = os.path.join(transcripts_dir, 'segments')\n",
    "        text_f = os.path.join(transcripts_dir, 'text')\n",
    "        # build dicts\n",
    "        wav_map = dict(line.strip().split(' ', 1) for line in open(wav_scp))\n",
    "        segments = {u: (r, float(s), float(e)) for u, r, s, e in (l.split() for l in open(segments_f))}\n",
    "        transcripts = {}\n",
    "        for line in open(text_f):\n",
    "            parts = line.strip().split(' ', 1)\n",
    "            transcripts[parts[0]] = parts[1] if len(parts) > 1 else ''\n",
    "        # write manifest entries\n",
    "        for utt, (rec, st, ed) in segments.items():\n",
    "            entry = {\n",
    "                'id': utt,\n",
    "                'audio': {\n",
    "                    'path': os.path.join(transcripts_dir, wav_map[rec]),\n",
    "                    'offset': st,\n",
    "                    'duration': ed - st\n",
    "                },\n",
    "                'transcript': transcripts.get(utt, '')\n",
    "            }\n",
    "            outf.write(json.dumps(entry) + '\\n')\n",
    "    print(f\"Written manifest: {manifest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef160d",
   "metadata": {},
   "source": [
    "# Load dataset + init Whisper-small + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e756d3a3-f410-416c-8e48-610cc26108be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330d35a937f344378d230c048bc76d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_split split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501a9694d8324a99a3cdc7c64e44c61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_split split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 768)\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {split: os.path.join(OUT_DIR, f\"{split}.jsonl\") for split in ['train_split', 'test_split']}\n",
    "ds = load_dataset('json', data_files=data_files, split=None)\n",
    "# Cast audio column to HF Audio feature\n",
    "for split in ds:\n",
    "    ds[split] = ds[split].cast_column('audio', Audio(sampling_rate=16000))\n",
    "\n",
    "# Initialize Whisper model & processor\n",
    "model_name = 'openai/whisper-small'\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language='hindi', task='transcribe')\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "# Freeze base model params\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Set decoder prompt IDs\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language='hindi', task='transcribe')\n",
    "\n",
    "# Configure and apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj']\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2c0d4",
   "metadata": {},
   "source": [
    "# Preprocess → features & labels\n",
    "\n",
    "Does: preprocess() extracts log-Mel features via processor.feature_extractor and tokenizes transcripts; maps to train_ds & test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38b27c90-69e7-4d08-b85c-701259113f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de305d7e006b4545a84f9073f569f596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7074ba2e80340bfba0ece5a02872ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(batch):\n",
    "    audio = batch['audio']\n",
    "    # extract input features\n",
    "    feats = processor.feature_extractor(\n",
    "        audio['array'], sampling_rate=audio['sampling_rate'], return_tensors='pt'\n",
    "    ).input_features[0]\n",
    "    # tokenize transcripts\n",
    "    labels = processor.tokenizer(\n",
    "        batch['transcript'], return_tensors='pt', padding=False\n",
    "    ).input_ids[0]\n",
    "    return {'input_features': feats, 'labels': labels}\n",
    "\n",
    "# Map preprocessing\n",
    "train_ds = ds['train_split'].map(preprocess, remove_columns=ds['train_split'].column_names)\n",
    "test_ds  = ds['test_split'].map(preprocess,  remove_columns=ds['test_split'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd027bb",
   "metadata": {},
   "source": [
    "# Collate v1 (pad features & labels)\n",
    "\n",
    "Does: Custom collate_fn that pads input_features to same time length and pads labels to same length with -100 (CTC/seq2seq loss ignore index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "921ae72d-d243-45a2-9676-d3c368e8977c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def collate_fn(batch):\n",
    "    import torch\n",
    "    # Convert lists to tensors and collect feats\n",
    "    feats = []\n",
    "    for b in batch:\n",
    "        f = b['input_features']\n",
    "        # convert to tensor\n",
    "        if not torch.is_tensor(f):\n",
    "            f = torch.tensor(f, dtype=torch.float)\n",
    "        feats.append(f)\n",
    "    # Pad feats\n",
    "    max_t = max(f.shape[0] for f in feats)\n",
    "    feat_dim = feats[0].shape[1]\n",
    "    padded_feats = []\n",
    "    for f in feats:\n",
    "        pad_len = max_t - f.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad_tensor = torch.zeros((pad_len, feat_dim), dtype=f.dtype)\n",
    "            f = torch.cat([f, pad_tensor], dim=0)\n",
    "        padded_feats.append(f)\n",
    "    input_features = torch.stack(padded_feats)\n",
    "    # Process labels\n",
    "    labels = []\n",
    "    for b in batch:\n",
    "        l = b['labels']\n",
    "        if not torch.is_tensor(l):\n",
    "            l = torch.tensor(l, dtype=torch.long)\n",
    "        labels.append(l)\n",
    "    max_l = max(l.shape[0] for l in labels)\n",
    "    padded_labels = []\n",
    "    for l in labels:\n",
    "        pad_len = max_l - l.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad_tensor = torch.full((pad_len,), -100, dtype=l.dtype)\n",
    "            l = torch.cat([l, pad_tensor], dim=0)\n",
    "        padded_labels.append(l)\n",
    "    labels = torch.stack(padded_labels)\n",
    "    return {'input_features': input_features, 'labels': labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd7c79",
   "metadata": {},
   "source": [
    "# Train v1 (no attention_mask) + eval\n",
    "\n",
    "Does: Builds DataLoaders; AdamW + linear warmup; unwraps underlying Whisper module; trains 5 epochs; saves outputs/whisper_lora_trained.pth; generates on test; computes WER (used as MER proxy) with jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65e05708-44c9-44c0-bebd-14de24f549ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 avg loss: 3.2596\n",
      "Epoch 2/5 avg loss: 3.2820\n",
      "Epoch 3/5 avg loss: 3.0617\n",
      "Epoch 4/5 avg loss: 3.0248\n",
      "Epoch 5/5 avg loss: 2.9147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test WER/MER: 509.50%\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def collate_fn(batch):\n",
    "    import torch\n",
    "    feats = []\n",
    "    for b in batch:\n",
    "        f = b['input_features']\n",
    "        if not torch.is_tensor(f): f = torch.tensor(f, dtype=torch.float)\n",
    "        feats.append(f)\n",
    "    max_t = max(f.shape[0] for f in feats)\n",
    "    feat_dim = feats[0].shape[1]\n",
    "    padded_feats = []\n",
    "    for f in feats:\n",
    "        pad_len = max_t - f.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad_tensor = torch.zeros((pad_len, feat_dim), dtype=f.dtype)\n",
    "            f = torch.cat([f, pad_tensor], dim=0)\n",
    "        padded_feats.append(f)\n",
    "    input_features = torch.stack(padded_feats)\n",
    "    labels = []\n",
    "    for b in batch:\n",
    "        l = b['labels']\n",
    "        if not torch.is_tensor(l): l = torch.tensor(l, dtype=torch.long)\n",
    "        labels.append(l)\n",
    "    max_l = max(l.shape[0] for l in labels)\n",
    "    padded_labels = []\n",
    "    for l in labels:\n",
    "        pad_len = max_l - l.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad_tensor = torch.full((pad_len,), -100, dtype=l.dtype)\n",
    "            l = torch.cat([l, pad_tensor], dim=0)\n",
    "        padded_labels.append(l)\n",
    "    labels = torch.stack(padded_labels)\n",
    "    return {'input_features': input_features, 'labels': labels}\n",
    "\n",
    "# Prepare DataLoader, optimizer, scheduler\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 8\n",
    "dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "sched = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=len(dl_train)*5)\n",
    "\n",
    "# Determine underlying Whisper model to avoid wrapper issues\n",
    "if hasattr(model, 'model'):\n",
    "    whisper_model = model.model\n",
    "elif hasattr(model, 'base_model'):\n",
    "    whisper_model = model.base_model\n",
    "else:\n",
    "    whisper_model = model\n",
    "whisper_model.to(device)\n",
    "whisper_model.train()\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs:=epochs if 'epochs' in locals() else 5):\n",
    "    total_loss = 0.0\n",
    "    for batch in dl_train:\n",
    "        optimizer.zero_grad()\n",
    "        feats = batch['input_features'].to(device)\n",
    "        labs = batch['labels'].to(device)\n",
    "        out = whisper_model(input_features=feats, labels=labs)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sched.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} avg loss: {total_loss/len(dl_train):.4f}\")\n",
    "\n",
    "# Save model state\n",
    "torch.save(model.state_dict(), 'outputs/whisper_lora_trained.pth')\n",
    "\n",
    "# Evaluation on test set\n",
    "dl_test = DataLoader(test_ds, batch_size=4, collate_fn=collate_fn)\n",
    "whisper_model.eval()\n",
    "preds, refs = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in dl_test:\n",
    "        feats = batch['input_features'].to(device)\n",
    "        gen_ids = whisper_model.generate(input_features=feats)\n",
    "        preds.extend(processor.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "        refs.extend(processor.batch_decode(batch['labels'], skip_special_tokens=True))\n",
    "# Compute WER as MER proxy\n",
    "wer_score = jiwer.wer(refs, preds)\n",
    "print(f\"Test WER/MER: {wer_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "782fb413-58af-405c-9639-57be044a9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changes attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d86dfb",
   "metadata": {},
   "source": [
    "# Collate v2 (adds attention_mask)\n",
    "\n",
    "Does: Rewrites collate_fn to also create a boolean attention_mask aligned with padded input_features; pads labels with -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32f1d930-8ea4-4c7e-891e-a9012d37ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def collate_fn(batch):\n",
    "    import torch\n",
    "    # Convert and collect input_features\n",
    "    feats = []\n",
    "    masks = []\n",
    "    for b in batch:\n",
    "        f = b['input_features']\n",
    "        # to tensor\n",
    "        if not torch.is_tensor(f): f = torch.tensor(f, dtype=torch.float)\n",
    "        feats.append(f)\n",
    "    # pad feats and build attention_mask\n",
    "    max_t = max(f.shape[0] for f in feats)\n",
    "    feat_dim = feats[0].shape[1]\n",
    "    padded_feats = []\n",
    "    for f in feats:\n",
    "        seq_len = f.shape[0]\n",
    "        pad_len = max_t - seq_len\n",
    "        # pad frames\n",
    "        if pad_len > 0:\n",
    "            pad = torch.zeros((pad_len, feat_dim), dtype=f.dtype)\n",
    "            pf = torch.cat([f, pad], dim=0)\n",
    "        else:\n",
    "            pf = f\n",
    "        padded_feats.append(pf)\n",
    "        # mask: 1 for real, 0 for pad\n",
    "        mask = torch.cat([torch.ones(seq_len, dtype=torch.bool), torch.zeros(max_t-seq_len, dtype=torch.bool)])\n",
    "        masks.append(mask)\n",
    "    input_features = torch.stack(padded_feats)\n",
    "    attention_mask = torch.stack(masks)\n",
    "    # Process labels\n",
    "    labels = []\n",
    "    for b in batch:\n",
    "        l = b['labels']\n",
    "        if not torch.is_tensor(l): l = torch.tensor(l, dtype=torch.long)\n",
    "        labels.append(l)\n",
    "    max_l = max(l.shape[0] for l in labels)\n",
    "    padded_labels = []\n",
    "    for l in labels:\n",
    "        pad_len = max_l - l.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad = torch.full((pad_len,), -100, dtype=l.dtype)\n",
    "            pl = torch.cat([l, pad], dim=0)\n",
    "        else:\n",
    "            pl = l\n",
    "        padded_labels.append(pl)\n",
    "    labels = torch.stack(padded_labels)\n",
    "    return {'input_features': input_features, 'attention_mask': attention_mask, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83399179",
   "metadata": {},
   "source": [
    "# Train v2 (with attention_mask) + eval\n",
    "\n",
    "Does: Same loop as Cell 9 but (intended to) pass attention_mask (note: this cell’s loop still only shows input_features in generate()); evaluates WER/MER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c715bd0f-68d2-4e67-a30e-78caa33dc133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 avg loss: 2.8533\n",
      "Epoch 2/5 avg loss: 2.8856\n",
      "Epoch 3/5 avg loss: 2.7823\n",
      "Epoch 4/5 avg loss: 2.8041\n",
      "Epoch 5/5 avg loss: 2.6986\n",
      "Test WER/MER: 721.39%\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def collate_fn(batch):\n",
    "    import torch\n",
    "    feats = []\n",
    "    for b in batch:\n",
    "        f = b['input_features']\n",
    "        if not torch.is_tensor(f): f = torch.tensor(f, dtype=torch.float)\n",
    "        feats.append(f)\n",
    "    max_t = max(f.shape[0] for f in feats)\n",
    "    feat_dim = feats[0].shape[1]\n",
    "    padded_feats = []\n",
    "    for f in feats:\n",
    "        pad_len = max_t - f.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad_tensor = torch.zeros((pad_len, feat_dim), dtype=f.dtype)\n",
    "            f = torch.cat([f, pad_tensor], dim=0)\n",
    "        padded_feats.append(f)\n",
    "    input_features = torch.stack(padded_feats)\n",
    "    labels = []\n",
    "    for b in batch:\n",
    "        l = b['labels']\n",
    "        if not torch.is_tensor(l): l = torch.tensor(l, dtype=torch.long)\n",
    "        labels.append(l)\n",
    "    max_l = max(l.shape[0] for l in labels)\n",
    "    padded_labels = []\n",
    "    for l in labels:\n",
    "        pad_len = max_l - l.shape[0]\n",
    "        if pad_len > 0:\n",
    "            pad_tensor = torch.full((pad_len,), -100, dtype=l.dtype)\n",
    "            l = torch.cat([l, pad_tensor], dim=0)\n",
    "        padded_labels.append(l)\n",
    "    labels = torch.stack(padded_labels)\n",
    "    return {'input_features': input_features, 'labels': labels}\n",
    "\n",
    "# Prepare DataLoader, optimizer, scheduler\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 8\n",
    "dl_train = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "sched = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=len(dl_train)*5)\n",
    "\n",
    "# Determine underlying Whisper model to avoid wrapper issues\n",
    "if hasattr(model, 'model'):\n",
    "    whisper_model = model.model\n",
    "elif hasattr(model, 'base_model'):\n",
    "    whisper_model = model.base_model\n",
    "else:\n",
    "    whisper_model = model\n",
    "whisper_model.to(device)\n",
    "whisper_model.train()\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs:=epochs if 'epochs' in locals() else 5):\n",
    "    total_loss = 0.0\n",
    "    for batch in dl_train:\n",
    "        optimizer.zero_grad()\n",
    "        feats = batch['input_features'].to(device)\n",
    "        labs = batch['labels'].to(device)\n",
    "        out = whisper_model(input_features=feats, labels=labs)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sched.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} avg loss: {total_loss/len(dl_train):.4f}\")\n",
    "\n",
    "# Save model state\n",
    "torch.save(model.state_dict(), 'outputs/whisper_lora_trained.pth')\n",
    "\n",
    "# Evaluation on test set\n",
    "dl_test = DataLoader(test_ds, batch_size=4, collate_fn=collate_fn)\n",
    "whisper_model.eval()\n",
    "preds, refs = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in dl_test:\n",
    "        feats = batch['input_features'].to(device)\n",
    "        gen_ids = whisper_model.generate(input_features=feats)\n",
    "        preds.extend(processor.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "        refs.extend(processor.batch_decode(batch['labels'], skip_special_tokens=True))\n",
    "# Compute WER as MER proxy\n",
    "wer_score = jiwer.wer(refs, preds)\n",
    "print(f\"Test WER/MER: {wer_score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa35d12-5607-432a-99b7-1383f15a895e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1eb6d50",
   "metadata": {},
   "source": [
    "# Consolidated pipeline (small + LoRA + attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1865ded2-9db1-47ef-b634-3f196c2a1e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote manifest: hf_manifests/train_split.jsonl\n",
      "Wrote manifest: hf_manifests/test_split.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98feaf899284498833e5e27b18c8aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_split split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f341d5bb45ac4b2ab24e7bc45ea28b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_split split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073aef4ad0b44f87808467979970b55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe083e4545c47bf965b5e748e790ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/209 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 4.6813\n",
      "Epoch 2 avg loss: 4.5931\n",
      "Epoch 3 avg loss: 4.5343\n",
      "Epoch 4 avg loss: 4.4618\n",
      "Epoch 5 avg loss: 4.1693\n",
      "Epoch 6 avg loss: 3.7791\n",
      "Epoch 7 avg loss: 3.4117\n",
      "Epoch 8 avg loss: 3.1597\n",
      "Epoch 9 avg loss: 2.9445\n",
      "Epoch 10 avg loss: 2.8437\n",
      "Test MER: 5.094958394517866\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# 2. Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import jiwer\n",
    "\n",
    "# %% [markdown]\n",
    "# 3. Data Manifest Generation\n",
    "\n",
    "# %%\n",
    "#DATA_ROOT = 'data'\n",
    "OUT_DIR = 'hf_manifests'\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "for split in ['train_split', 'test_split']:\n",
    "    transcripts_dir = os.path.join(split, 'transcripts')\n",
    "    manifest = os.path.join(OUT_DIR, f'{split}.jsonl')\n",
    "    with open(manifest, 'w') as fw:\n",
    "        wav = os.path.join(transcripts_dir, 'wav.scp')\n",
    "        seg = os.path.join(transcripts_dir, 'segments')\n",
    "        txt = os.path.join(transcripts_dir, 'text')\n",
    "        wav_map = dict(line.strip().split(' ',1) for line in open(wav))\n",
    "        segments = {u:(r,float(s),float(e)) for u,r,s,e in (l.split() for l in open(seg))}\n",
    "        transcripts = {}\n",
    "        for line in open(txt):\n",
    "            parts = line.strip().split(' ',1)\n",
    "            transcripts[parts[0]] = parts[1] if len(parts)>1 else ''\n",
    "        for utt,(rec,st,ed) in segments.items():\n",
    "            entry = {\n",
    "                'id': utt,\n",
    "                'audio': {'path': os.path.join(transcripts_dir, wav_map[rec]), 'offset': st, 'duration': ed-st},\n",
    "                'transcript': transcripts.get(utt,'')\n",
    "            }\n",
    "            fw.write(json.dumps(entry)+'\\n')\n",
    "    print(f'Wrote manifest: {manifest}')\n",
    "\n",
    "# 4. Load Data & Initialize Model\n",
    "# Load HF dataset, initialize Whisper, freeze base, and apply LoRA. Ensure transcription mode.\n",
    "\n",
    "# %%\n",
    "# Dataset\n",
    "files = {s: os.path.join(OUT_DIR, f'{s}.jsonl') for s in ['train_split','test_split']}\n",
    "ds = load_dataset('json', data_files=files, split=None)\n",
    "for s in ds: ds[s] = ds[s].cast_column('audio', Audio(sampling_rate=16000))\n",
    "# Processor & Model\n",
    "model_nm = 'openai/whisper-small'\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    model_nm,\n",
    "    language='hindi',   # force transcription, not translation\n",
    "    task='transcribe'\n",
    ")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_nm)\n",
    "# Freeze parameters\n",
    "for p in model.parameters(): p.requires_grad=False\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language='hindi', task='transcribe')\n",
    "# LoRA adapters\n",
    "lora_cfg = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.1, target_modules=['q_proj','v_proj'])\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#5. Preprocessing & Collation\n",
    "#Extract features, tokenize, and build attention masks.\n",
    "\n",
    "# %%\n",
    "def preprocess(batch):\n",
    "    arr = batch['audio']['array']\n",
    "    sr  = batch['audio']['sampling_rate']\n",
    "    feats = processor.feature_extractor(arr, sampling_rate=sr, return_tensors='pt').input_features[0]\n",
    "    labs  = processor.tokenizer(batch['transcript'], return_tensors='pt', padding=False).input_ids[0]\n",
    "    return {'input_features': feats, 'labels': labs}\n",
    "\n",
    "train_ds = ds['train_split'].map(preprocess, remove_columns=ds['train_split'].column_names)\n",
    "test_ds  = ds['test_split'].map(preprocess,  remove_columns=ds['test_split'].column_names)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    import torch\n",
    "    feats, masks, labels = [], [], []\n",
    "    # prepare features and mask\n",
    "    for b in batch:\n",
    "        f = b['input_features']\n",
    "        if not torch.is_tensor(f): f = torch.tensor(f)\n",
    "        feats.append(f);\n",
    "    max_t = max(f.shape[0] for f in feats)\n",
    "    feat_dim = feats[0].shape[1]\n",
    "    for f in feats:\n",
    "        pad = torch.zeros((max_t - f.shape[0], feat_dim), dtype=f.dtype)\n",
    "        feats_p = torch.cat([f, pad], dim=0)\n",
    "        mask = torch.cat([torch.ones(f.shape[0], dtype=torch.bool), torch.zeros(max_t-f.shape[0], dtype=torch.bool)])\n",
    "        masks.append(mask)\n",
    "        feats[padded_feats.index(f)] if False else None\n",
    "    input_features = torch.stack([torch.cat([f, torch.zeros((max_t-f.shape[0],feat_dim),dtype=f.dtype)],dim=0) for f in feats])\n",
    "    attention_mask  = torch.stack(masks)\n",
    "    # prepare labels\n",
    "    for b in batch:\n",
    "        l = b['labels']\n",
    "        if not torch.is_tensor(l): l = torch.tensor(l)\n",
    "        labels.append(l)\n",
    "    max_l = max(l.shape[0] for l in labels)\n",
    "    labels = torch.stack([torch.cat([l, torch.full((max_l-l.shape[0],), -100, dtype=l.dtype)], dim=0) for l in labels])\n",
    "    return {'input_features': input_features, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "#6. Manual Training & Generation\n",
    "# Use scheduler, include attention_mask in forward and generate.\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dl_train = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "dl_test  = DataLoader(test_ds, batch_size=4, collate_fn=collate_fn)\n",
    "# Optimizer & scheduler\n",
    "opt = AdamW(model.parameters(), lr=5e-5)\n",
    "steps = len(dl_train)*10  # 10 epochs improving\n",
    "sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=500, num_training_steps=steps)\n",
    "# Underlying model bypass\n",
    "base = model.model if hasattr(model,'model') else model.base_model if hasattr(model,'base_model') else model\n",
    "base.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    base.train(); tot=0.0\n",
    "    for b in dl_train:\n",
    "        opt.zero_grad()\n",
    "        feats = b['input_features'].to(device)\n",
    "        mask  = b['attention_mask'].to(device)\n",
    "        labs  = b['labels'].to(device)\n",
    "        out = base(input_features=feats, attention_mask=mask, labels=labs)\n",
    "        loss = out.loss; loss.backward(); opt.step(); sched.step(); tot+=loss.item()\n",
    "    print(f\"Epoch {epoch+1} avg loss: {tot/len(dl_train):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "tot_preds, tot_refs = [], []\n",
    "base.eval()\n",
    "with torch.no_grad():\n",
    "    for b in dl_test:\n",
    "        feats = b['input_features'].to(device)\n",
    "        mask  = b['attention_mask'].to(device)\n",
    "        gen_ids = base.generate(input_features=feats, attention_mask=mask)\n",
    "        tot_preds.extend(processor.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "        tot_refs.extend(processor.batch_decode(b['labels'], skip_special_tokens=True))\n",
    "# WER/CER\n",
    "print(\"Test MER:\", jiwer.wer(tot_refs, tot_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d206f-8cab-4bb6-9af5-5f5b0ac0b3c7",
   "metadata": {},
   "source": [
    "# I’ve replaced the collate_fn with a corrected implementation that:\n",
    "\n",
    "Properly converts lists to tensors,\n",
    "\n",
    "Pads input_features and builds attention_mask,\n",
    "\n",
    "Pads labels with -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a44c025c-c891-4624-98b2-61e99a224eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 2.8489\n",
      "Epoch 2 avg loss: 2.8409\n",
      "Epoch 3 avg loss: 2.8025\n",
      "Epoch 4 avg loss: 2.7719\n",
      "Epoch 5 avg loss: 2.7238\n",
      "Epoch 6 avg loss: 2.6938\n",
      "Epoch 7 avg loss: 2.6426\n",
      "Epoch 8 avg loss: 2.5853\n",
      "Epoch 9 avg loss: 2.5721\n",
      "Epoch 10 avg loss: 2.5139\n",
      "Test MER: 15.167400881057269\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    import torch\n",
    "    # Collect input_features and build attention masks\n",
    "    feats, labels_list = [], []\n",
    "    for b in batch:\n",
    "        f = b['input_features']\n",
    "        if not torch.is_tensor(f):\n",
    "            f = torch.tensor(f, dtype=torch.float)\n",
    "        feats.append(f)\n",
    "        l = b['labels']\n",
    "        if not torch.is_tensor(l):\n",
    "            l = torch.tensor(l, dtype=torch.long)\n",
    "        labels_list.append(l)\n",
    "    # Pad features\n",
    "    max_t = max(f.shape[0] for f in feats)\n",
    "    feat_dim = feats[0].shape[1]\n",
    "    padded_feats, masks = [], []\n",
    "    for f in feats:\n",
    "        pad_len = max_t - f.shape[0]\n",
    "        pad_feat = torch.zeros((pad_len, feat_dim), dtype=f.dtype)\n",
    "        padded = torch.cat([f, pad_feat], dim=0)\n",
    "        padded_feats.append(padded)\n",
    "        mask = torch.cat([torch.ones(f.shape[0], dtype=torch.bool), torch.zeros(pad_len, dtype=torch.bool)])\n",
    "        masks.append(mask)\n",
    "    input_features = torch.stack(padded_feats)\n",
    "    attention_mask = torch.stack(masks)\n",
    "    # Pad labels with -100\n",
    "    max_l = max(l.shape[0] for l in labels_list)\n",
    "    padded_labels = []\n",
    "    for l in labels_list:\n",
    "        pad_len = max_l - l.shape[0]\n",
    "        pad_label = torch.full((pad_len,), -100, dtype=l.dtype)\n",
    "        padded_labels.append(torch.cat([l, pad_label], dim=0))\n",
    "    labels = torch.stack(padded_labels)\n",
    "    return {'input_features': input_features, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# %% [markdown]\n",
    "#6. Manual Training & Generation\n",
    "\n",
    "# %%\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dl_train = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "dl_test  = DataLoader(test_ds, batch_size=4, collate_fn=collate_fn)\n",
    "# Optimizer & scheduler\n",
    "opt = AdamW(model.parameters(), lr=5e-5)\n",
    "steps = len(dl_train)*10  # 10 epochs improving\n",
    "sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=500, num_training_steps=steps)\n",
    "# Underlying model bypass\n",
    "base = model.model if hasattr(model,'model') else model.base_model if hasattr(model,'base_model') else model\n",
    "base.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    base.train(); tot=0.0\n",
    "    for b in dl_train:\n",
    "        opt.zero_grad()\n",
    "        feats = b['input_features'].to(device)\n",
    "        mask  = b['attention_mask'].to(device)\n",
    "        labs  = b['labels'].to(device)\n",
    "        out = base(input_features=feats, attention_mask=mask, labels=labs)\n",
    "        loss = out.loss; loss.backward(); opt.step(); sched.step(); tot+=loss.item()\n",
    "    print(f\"Epoch {epoch+1} avg loss: {tot/len(dl_train):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "tot_preds, tot_refs = [], []\n",
    "base.eval()\n",
    "with torch.no_grad():\n",
    "    for b in dl_test:\n",
    "        feats = b['input_features'].to(device)\n",
    "        mask  = b['attention_mask'].to(device)\n",
    "        gen_ids = base.generate(input_features=feats, attention_mask=mask)\n",
    "        tot_preds.extend(processor.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "        tot_refs.extend(processor.batch_decode(b['labels'], skip_special_tokens=True))\n",
    "# WER/CER\n",
    "print(\"Test MER:\", jiwer.wer(tot_refs, tot_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b4145-2823-4861-81df-ec043cee2ab5",
   "metadata": {},
   "source": [
    "# I’ve integrated SpecAugment into the training loop—applying frequency and time masking at each batch—and bumped to 15 epochs with a lower LR of 3e-5. Run this updated code; let’s see the new MER. If we still need improvements, we can explore model scaling or LM rescoring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "955da8a8-7d89-413c-b614-9b6fd0eb7162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 avg loss: 3.1173\n",
      "Epoch 2/15 avg loss: 3.0652\n",
      "Epoch 3/15 avg loss: 3.0307\n",
      "Epoch 4/15 avg loss: 2.9093\n",
      "Epoch 5/15 avg loss: 3.0522\n",
      "Epoch 6/15 avg loss: 2.8615\n",
      "Epoch 7/15 avg loss: 2.9415\n",
      "Epoch 8/15 avg loss: 2.9710\n",
      "Epoch 9/15 avg loss: 2.7921\n",
      "Epoch 10/15 avg loss: 2.7843\n",
      "Epoch 11/15 avg loss: 2.7789\n",
      "Epoch 12/15 avg loss: 2.6767\n",
      "Epoch 13/15 avg loss: 2.6572\n",
      "Epoch 14/15 avg loss: 2.6256\n",
      "Epoch 15/15 avg loss: 2.5515\n",
      "Test MER: 1.066079295154185\n",
      "Test MER: 1.066079295154185\n"
     ]
    }
   ],
   "source": [
    "# 6. Manual Training & Generation\n",
    "# Use scheduler, include attention_mask in forward and generate.\n",
    "\n",
    "# %%\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dl_train = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "dl_test  = DataLoader(test_ds, batch_size=4, collate_fn=collate_fn)\n",
    "# Optimizer & scheduler\n",
    "opt = AdamW(model.parameters(), lr=5e-5)\n",
    "steps = len(dl_train)*10  # 10 epochs improving\n",
    "sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=500, num_training_steps=steps)\n",
    "# Underlying model bypass\n",
    "base = model.model if hasattr(model,'model') else model.base_model if hasattr(model,'base_model') else model\n",
    "base.to(device)\n",
    "\n",
    "# %% [markdown]\n",
    "## 6. Manual Training & Generation (with SpecAugment)\n",
    "# Use scheduler, include attention_mask in forward and generate, and apply SpecAugment to training audio features.\n",
    "\n",
    "# %%\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# SpecAugment transforms\n",
    "freq_mask = T.FrequencyMasking(freq_mask_param=27)\n",
    "time_mask = T.TimeMasking(time_mask_param=100)\n",
    "\n",
    "# DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dl_train = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "dl_test  = DataLoader(test_ds, batch_size=4, collate_fn=collate_fn)\n",
    "# Optimizer & scheduler\n",
    "opt = AdamW(model.parameters(), lr=3e-5)\n",
    "steps = len(dl_train)*15  # 15 epochs\n",
    "sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=500, num_training_steps=steps)\n",
    "# Underlying model bypass\n",
    "base = model.model if hasattr(model,'model') else model.base_model if hasattr(model,'base_model') else model\n",
    "base.to(device)\n",
    "\n",
    "# Training loop with SpecAugment\n",
    "epochs = 15\n",
    "for epoch in range(epochs:=epochs if 'epochs' in locals() else 15):\n",
    "    base.train(); tot_loss=0.0\n",
    "    for b in dl_train:\n",
    "        opt.zero_grad()\n",
    "        feats = b['input_features'].to(device)           # (B, T, D)\n",
    "        # Apply SpecAugment: need (B, D, T)\n",
    "        feats_aug = feats.permute(0,2,1)\n",
    "        feats_aug = freq_mask(feats_aug)\n",
    "        feats_aug = time_mask(feats_aug)\n",
    "        feats_aug = feats_aug.permute(0,2,1)\n",
    "        mask  = b['attention_mask'].to(device)\n",
    "        labs  = b['labels'].to(device)\n",
    "        out = base(input_features=feats_aug, attention_mask=mask, labels=labs)\n",
    "        loss = out.loss; loss.backward(); opt.step(); sched.step(); tot_loss+=loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} avg loss: {tot_loss/len(dl_train):.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 7. Evaluation\n",
    "\n",
    "# Evaluate without augmentation\n",
    "dl_test = DataLoader(test_ds, batch_size=4, collate_fn=collate_fn)\n",
    "base.eval(); preds, refs = [], []\n",
    "with torch.no_grad():\n",
    "    for b in dl_test:\n",
    "        feats = b['input_features'].to(device)\n",
    "        mask  = b['attention_mask'].to(device)\n",
    "        gen_ids = base.generate(input_features=feats, attention_mask=mask)\n",
    "        preds.extend(processor.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "        refs.extend(processor.batch_decode(b['labels'], skip_special_tokens=True))\n",
    "# Compute WER/CER\n",
    "print(\"Test MER:\", jiwer.wer(refs, preds))\n",
    "\n",
    "tot_preds, tot_refs = [], []\n",
    "base.eval()\n",
    "with torch.no_grad():\n",
    "    for b in dl_test:\n",
    "        feats = b['input_features'].to(device)\n",
    "        mask  = b['attention_mask'].to(device)\n",
    "        gen_ids = base.generate(input_features=feats, attention_mask=mask)\n",
    "        tot_preds.extend(processor.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "        tot_refs.extend(processor.batch_decode(b['labels'], skip_special_tokens=True))\n",
    "# WER/CER\n",
    "print(\"Test MER:\", jiwer.wer(tot_refs, tot_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2d0f8-c1c8-4000-a164-b36574ac2bf3",
   "metadata": {},
   "source": [
    "# I’ve upgraded the base model to openai/whisper-medium for stronger acoustic features—simply swap out “small” for “medium” when loading the processor and model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "542f0873-097c-48c8-a048-c6b070d2354e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 avg loss: 4.6990\n",
      "Epoch 2/15 avg loss: 4.5239\n",
      "Epoch 3/15 avg loss: 4.6079\n",
      "Epoch 4/15 avg loss: 4.4906\n",
      "Epoch 5/15 avg loss: 4.3763\n",
      "Epoch 6/15 avg loss: 4.2706\n",
      "Epoch 7/15 avg loss: 4.1671\n",
      "Epoch 8/15 avg loss: 3.7595\n",
      "Epoch 9/15 avg loss: 3.5597\n",
      "Epoch 10/15 avg loss: 3.5794\n",
      "Epoch 11/15 avg loss: 3.4650\n",
      "Epoch 12/15 avg loss: 3.3509\n",
      "Epoch 13/15 avg loss: 3.2239\n",
      "Epoch 14/15 avg loss: 3.1868\n",
      "Epoch 15/15 avg loss: 3.0129\n",
      "Test MER: 3.3416544297601565\n",
      "Test MER: 3.3416544297601565\n"
     ]
    }
   ],
   "source": [
    "# 4. Load Data & Initialize Model\n",
    "\n",
    "# %%\n",
    "# Dataset\n",
    "files = {s: os.path.join(OUT_DIR, f'{s}.jsonl') for s in ['train_split','test_split']}\n",
    "ds = load_dataset('json', data_files=files, split=None)\n",
    "for s in ds: ds[s] = ds[s].cast_column('audio', Audio(sampling_rate=16000))\n",
    "# Processor & Model\n",
    "model_nm = 'openai/whisper-medium'  # Upgrade to medium for richer representations\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    model_nm,\n",
    "    language='hindi',   # force transcription, not translation\n",
    "    task='transcribe'\n",
    ")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_nm)\n",
    "# Freeze parameters\n",
    "for p in model.parameters(): p.requires_grad=False\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language='hindi', task='transcribe')\n",
    "# LoRA adapters\n",
    "lora_cfg = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.1, target_modules=['q_proj','v_proj'])\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# %% [markdown]\n",
    "# 5. Preprocessing & Collation\n",
    "# Extract features, tokenize, and build attention masks.\n",
    "\n",
    "# %%\n",
    "def preprocess(batch):\n",
    "    arr = batch['audio']['array']\n",
    "    sr  = batch['audio']['sampling_rate']\n",
    "    feats = processor.feature_extractor(arr, sampling_rate=sr, return_tensors='pt').input_features[0]\n",
    "    labs  = processor.tokenizer(batch['transcript'], return_tensors='pt', padding=False).input_ids[0]\n",
    "    return {'input_features': feats, 'labels': labs}\n",
    "\n",
    "train_ds = ds['train_split'].map(preprocess, remove_columns=ds['train_split'].column_names)\n",
    "test_ds  = ds['test_split'].map(preprocess,  remove_columns=ds['test_split'].column_names)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    import torch\n",
    "    # Collect input_features and build attention masks\n",
    "    feats, labels_list = [], []\n",
    "    for b in batch:\n",
    "        f = b['input_features']\n",
    "        if not torch.is_tensor(f):\n",
    "            f = torch.tensor(f, dtype=torch.float)\n",
    "        feats.append(f)\n",
    "        l = b['labels']\n",
    "        if not torch.is_tensor(l):\n",
    "            l = torch.tensor(l, dtype=torch.long)\n",
    "        labels_list.append(l)\n",
    "    # Pad features\n",
    "    max_t = max(f.shape[0] for f in feats)\n",
    "    feat_dim = feats[0].shape[1]\n",
    "    padded_feats, masks = [], []\n",
    "    for f in feats:\n",
    "        pad_len = max_t - f.shape[0]\n",
    "        pad_feat = torch.zeros((pad_len, feat_dim), dtype=f.dtype)\n",
    "        padded = torch.cat([f, pad_feat], dim=0)\n",
    "        padded_feats.append(padded)\n",
    "        mask = torch.cat([torch.ones(f.shape[0], dtype=torch.bool), torch.zeros(pad_len, dtype=torch.bool)])\n",
    "        masks.append(mask)\n",
    "    input_features = torch.stack(padded_feats)\n",
    "    attention_mask = torch.stack(masks)\n",
    "    # Pad labels with -100\n",
    "    max_l = max(l.shape[0] for l in labels_list)\n",
    "    padded_labels = []\n",
    "    for l in labels_list:\n",
    "        pad_len = max_l - l.shape[0]\n",
    "        pad_label = torch.full((pad_len,), -100, dtype=l.dtype)\n",
    "        padded_labels.append(torch.cat([l, pad_label], dim=0))\n",
    "    labels = torch.stack(padded_labels)\n",
    "    return {'input_features': input_features, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# %% [markdown]\n",
    "# 6. Manual Training & Generation\n",
    "# Use scheduler, include attention_mask in forward and generate.\n",
    "\n",
    "# %%\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dl_train = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "dl_test  = DataLoader(test_ds, batch_size=4, collate_fn=collate_fn)\n",
    "# Optimizer & scheduler\n",
    "opt = AdamW(model.parameters(), lr=5e-5)\n",
    "steps = len(dl_train)*10  # 10 epochs improving\n",
    "sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=500, num_training_steps=steps)\n",
    "# Underlying model bypass\n",
    "base = model.model if hasattr(model,'model') else model.base_model if hasattr(model,'base_model') else model\n",
    "base.to(device)\n",
    "\n",
    "# %% [markdown]\n",
    "## 6. Manual Training & Generation (with SpecAugment)\n",
    "# Use scheduler, include attention_mask in forward and generate, and apply SpecAugment to training audio features.\n",
    "\n",
    "# %%\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# SpecAugment transforms\n",
    "freq_mask = T.FrequencyMasking(freq_mask_param=27)\n",
    "time_mask = T.TimeMasking(time_mask_param=100)\n",
    "\n",
    "# DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dl_train = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "dl_test  = DataLoader(test_ds, batch_size=4, collate_fn=collate_fn)\n",
    "# Optimizer & scheduler\n",
    "opt = AdamW(model.parameters(), lr=3e-5)\n",
    "steps = len(dl_train)*15  # 15 epochs\n",
    "sched = get_linear_schedule_with_warmup(opt, num_warmup_steps=500, num_training_steps=steps)\n",
    "# Underlying model bypass\n",
    "base = model.model if hasattr(model,'model') else model.base_model if hasattr(model,'base_model') else model\n",
    "base.to(device)\n",
    "\n",
    "# Training loop with SpecAugment\n",
    "epochs = 15\n",
    "for epoch in range(epochs:=epochs if 'epochs' in locals() else 15):\n",
    "    base.train(); tot_loss=0.0\n",
    "    for b in dl_train:\n",
    "        opt.zero_grad()\n",
    "        feats = b['input_features'].to(device)           # (B, T, D)\n",
    "        # Apply SpecAugment: need (B, D, T)\n",
    "        feats_aug = feats.permute(0,2,1)\n",
    "        feats_aug = freq_mask(feats_aug)\n",
    "        feats_aug = time_mask(feats_aug)\n",
    "        feats_aug = feats_aug.permute(0,2,1)\n",
    "        mask  = b['attention_mask'].to(device)\n",
    "        labs  = b['labels'].to(device)\n",
    "        out = base(input_features=feats_aug, attention_mask=mask, labels=labs)\n",
    "        loss = out.loss; loss.backward(); opt.step(); sched.step(); tot_loss+=loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} avg loss: {tot_loss/len(dl_train):.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "## 7. Evaluation\n",
    "\n",
    "# Evaluate without augmentation\n",
    "dl_test = DataLoader(test_ds, batch_size=4, collate_fn=collate_fn)\n",
    "base.eval(); preds, refs = [], []\n",
    "with torch.no_grad():\n",
    "    for b in dl_test:\n",
    "        feats = b['input_features'].to(device)\n",
    "        mask  = b['attention_mask'].to(device)\n",
    "        gen_ids = base.generate(input_features=feats, attention_mask=mask)\n",
    "        preds.extend(processor.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "        refs.extend(processor.batch_decode(b['labels'], skip_special_tokens=True))\n",
    "# Compute WER/CER\n",
    "print(\"Test MER:\", jiwer.wer(refs, preds))\n",
    "\n",
    "tot_preds, tot_refs = [], []\n",
    "base.eval()\n",
    "with torch.no_grad():\n",
    "    for b in dl_test:\n",
    "        feats = b['input_features'].to(device)\n",
    "        mask  = b['attention_mask'].to(device)\n",
    "        gen_ids = base.generate(input_features=feats, attention_mask=mask)\n",
    "        tot_preds.extend(processor.batch_decode(gen_ids, skip_special_tokens=True))\n",
    "        tot_refs.extend(processor.batch_decode(b['labels'], skip_special_tokens=True))\n",
    "# WER/CER\n",
    "print(\"Test MER:\", jiwer.wer(tot_refs, tot_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f212d4-1807-4220-872a-a9a316167b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8d907-4278-43eb-9e37-14493e4be233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
