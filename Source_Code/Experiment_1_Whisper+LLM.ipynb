{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3abdc1-e51e-489b-8f66-8518df22eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted train_split.zip to /\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "zip_path = 'train_split.zip'\n",
    "extract_dir = ''\n",
    "if not os.path.exists(extract_dir):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall(extract_dir)\n",
    "    print(f\"Extracted {zip_path} to {extract_dir}/\")\n",
    "else:\n",
    "    print(f\"Dataset directory already exists: {extract_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6991ee-4fb9-464e-bdc4-8af9b9e2c6c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai-whisper\n",
      "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Collecting numba (from openai-whisper)\n",
      "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.25.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.4.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.66.5)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from openai-whisper) (8.10.0)\n",
      "Collecting tiktoken (from openai-whisper)\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba->openai-whisper)\n",
      "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper) (12.6.77)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
      "Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803404 sha256=e333f00a5faf39e28c32c0909726d73f4b94666144e7ec08be893bd2fbd7bd3e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: llvmlite, tiktoken, numba, openai-whisper\n",
      "Successfully installed llvmlite-0.44.0 numba-0.61.2 openai-whisper-20240930 tiktoken-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai-whisper pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abd29479-b083-45fe-b069-2e5eac6f65f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./.local/lib/python3.11/site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.9.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (5.1.1)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.10.0)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (23.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.local/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m403.7/403.7 kB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.5.0.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.5/252.5 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: soxr, msgpack, lazy_loader, audioread, soundfile, pooch, librosa\n",
      "Successfully installed audioread-3.0.1 lazy_loader-0.4 librosa-0.11.0 msgpack-1.1.0 pooch-1.8.2 soundfile-0.13.1 soxr-0.5.0.post1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b519325",
   "metadata": {},
   "source": [
    "# Whisper transcription (no-ffmpeg, base model)\n",
    "    - What it does:\n",
    "        - Loads whisper (model \"base\").\n",
    "        - Reads Kaldi files from train_split/transcripts/{wav.scp, segments, text}.\n",
    "        - Loads audio via librosa, slices per segments, calls model.transcribe(y_seg, language=\"hi\", fp16=False).\n",
    "        -Saves CSV → outputs_noffmpeg/whisper_transcriptions_noffmpeg.csv with columns: utt_id, reference, whisper_output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892b4234-32ad-476b-ae40-e3945237b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transcription done. File saved at: outputs_noffmpeg/whisper_transcriptions_noffmpeg.csv\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(\"base\")  # or 'medium', 'large'\n",
    "\n",
    "# Paths\n",
    "base_path = Path(\"train_split/transcripts\")\n",
    "output_dir = Path(\"outputs_noffmpeg\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Read mapping files\n",
    "with open(base_path / \"wav.scp\") as f:\n",
    "    wav_scp = dict(line.strip().split() for line in f)\n",
    "\n",
    "with open(base_path / \"text\") as f:\n",
    "    ref_text = {line.split()[0]: \" \".join(line.strip().split()[1:]) for line in f}\n",
    "\n",
    "# Read segments\n",
    "segments = []\n",
    "with open(base_path / \"segments\") as f:\n",
    "    for line in f:\n",
    "        utt_id, wav_id, start, end = line.strip().split()\n",
    "        segments.append({\n",
    "            \"utt_id\": utt_id,\n",
    "            \"wav_path\": str(base_path / wav_scp[wav_id]),\n",
    "            \"start\": float(start),\n",
    "            \"end\": float(end)\n",
    "        })\n",
    "\n",
    "# Run transcription directly on NumPy audio\n",
    "results = []\n",
    "for seg in segments:\n",
    "    # Load full audio and extract segment\n",
    "    y, sr = librosa.load(seg[\"wav_path\"], sr=16000)\n",
    "    start_sample = int(seg[\"start\"] * sr)\n",
    "    end_sample = int(seg[\"end\"] * sr)\n",
    "    y_seg = y[start_sample:end_sample]\n",
    "\n",
    "    # Pass raw audio array to Whisper directly\n",
    "    result = model.transcribe(y_seg, language=\"hi\", fp16=False)\n",
    "\n",
    "    results.append({\n",
    "        \"utt_id\": seg[\"utt_id\"],\n",
    "        \"reference\": ref_text.get(seg[\"utt_id\"], \"\"),\n",
    "        \"whisper_output\": result[\"text\"].strip()\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(output_dir / \"whisper_transcriptions_noffmpeg.csv\", index=False)\n",
    "print(\"✅ Transcription done. File saved at:\", output_dir / \"whisper_transcriptions_noffmpeg.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46ebaa1-5927-4c23-9067-d77fd05ddfb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.45.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d2a3fe",
   "metadata": {},
   "source": [
    "# LLM refinement v1 (Qwen 1.5-1.8B) on Whisper CSV\n",
    "    - What it does:\n",
    "\n",
    "        - Loads outputs_noffmpeg/whisper_transcriptions_noffmpeg.csv.\n",
    "        - Creates a prompt: “preserve Hindi–English code-switching; don’t translate; keep Hindi in Devanagari; only light fixes.”\n",
    "        - Uses pipeline(\"text-generation\", model=\"Qwen/Qwen1.5-1.8B\", device_map=\"auto\", torch_dtype=float16); refines each whisper_output.\n",
    "        - Saves → outputs_noffmpeg/llm_refined_transcripts.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3560379-0cc5-464a-bf1a-7d291261a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM refinement completed and saved to: outputs_noffmpeg/llm_refined_transcripts.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "# Path to the output of your Whisper transcription\n",
    "csv_path = Path(\"outputs_noffmpeg/whisper_transcriptions_noffmpeg.csv\")\n",
    "output_path = Path(\"outputs_noffmpeg/llm_refined_transcripts.csv\")\n",
    "\n",
    "# HuggingFace LLM model (you can swap for Mixtral, DeepSeek, etc.)\n",
    "model_name = \"Qwen/Qwen1.5-1.8B\"\n",
    "# ----------------------------------------\n",
    "\n",
    "# ✅ Load Whisper transcription output\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# ✅ Load LLM with GPU acceleration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",  # Automatically maps to available GPU\n",
    "    torch_dtype=torch.float16  # Use float32 if float16 is unsupported\n",
    ")\n",
    "\n",
    "# ✅ Define LLM pipeline\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",   # Explicitly ensures GPU usage\n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "# ✅ Function to prompt and refine text\n",
    "def refine_text(raw):\n",
    "    prompt = f\"\"\"\n",
    "Refine the following Hindi-English code-switched sentence. Retain code-switching points and improve fluency and grammar.\n",
    "\n",
    "Original: {raw}\n",
    "Refined:\"\"\"\n",
    "    response = llm(prompt, num_return_sequences=1)[0][\"generated_text\"]\n",
    "    return response.split(\"Refined:\")[-1].strip()\n",
    "\n",
    "# ✅ Apply LLM to all rows\n",
    "df[\"llm_refined\"] = df[\"whisper_output\"].apply(refine_text)\n",
    "\n",
    "# ✅ Save final result\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"✅ LLM refinement completed and saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0c14e2-c150-4b69-8e21-166decb798e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting click>=8.1.8 (from jiwer)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, click, jiwer\n",
      "Successfully installed click-8.2.1 jiwer-3.1.0 rapidfuzz-3.13.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d065bf1",
   "metadata": {},
   "source": [
    "# Evaluate (WER/CER) for v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d581e2-d4c2-446b-a84d-94dc7a6d69c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Results:\n",
      "\n",
      "         Model    WER (%)    CER (%)\n",
      "Whisper Output 104.828494  90.138387\n",
      "   LLM Refined 469.010523 636.349431\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from jiwer import wer, cer\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"outputs_noffmpeg/llm_refined_transcripts.csv\")\n",
    "df = df.dropna(subset=[\"reference\", \"whisper_output\", \"llm_refined\"])\n",
    "\n",
    "# Compute row-wise WER & CER\n",
    "wer_whisper_scores = []\n",
    "wer_llm_scores = []\n",
    "cer_whisper_scores = []\n",
    "cer_llm_scores = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    wer_whisper_scores.append(wer(row[\"reference\"], row[\"whisper_output\"]))\n",
    "    wer_llm_scores.append(wer(row[\"reference\"], row[\"llm_refined\"]))\n",
    "    cer_whisper_scores.append(cer(row[\"reference\"], row[\"whisper_output\"]))\n",
    "    cer_llm_scores.append(cer(row[\"reference\"], row[\"llm_refined\"]))\n",
    "\n",
    "# Prepare results table\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Whisper Output\", \"LLM Refined\"],\n",
    "    \"WER (%)\": [\n",
    "        sum(wer_whisper_scores)/len(wer_whisper_scores)*100,\n",
    "        sum(wer_llm_scores)/len(wer_llm_scores)*100\n",
    "    ],\n",
    "    \"CER (%)\": [\n",
    "        sum(cer_whisper_scores)/len(cer_whisper_scores)*100,\n",
    "        sum(cer_llm_scores)/len(cer_llm_scores)*100\n",
    "    ]\n",
    "})\n",
    "\n",
    "# ✅ Print the comparison result\n",
    "print(\"\\n📊 Evaluation Results:\\n\")\n",
    "print(results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04bc250a-9114-417f-9f42-d7bd239fafa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      utt_id                                                                        reference                                                                          whisper_output                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      llm_refined\n",
      "210672_0jdhCdy6wPFMjRXl_0102                                   यहाँ विभिन्न tools हैं जो हम उपयोग कर सकते हैं                                                ँवके विर बू वस वार भाभीत आिए कर सकता hit                                                                                                                                                                                                                                                                                            उसे बू वस वार भाभीत आइए कर सकते हैं\\n\\nStep 1: Identify the code-switching points in the original sentence.\\n- \"विर बू वस वार\" (hit)\\n- \"भाभीत आइए\" (can)\\n\\nStep 2: Retain the code-switching points and improve flu\n",
      "384379_0a8GyoWn0KRgXxTx_0000 koha interface में how to add an item type पर spoken tutorial में आपका स्वागत है Kohha interface may, how to add an item type, Pulse Book on tutorial may, your welcome.                                                                                                                                                                                                                                                                                                                                                                                                          Kohha interface may be added to tutorial, how to add an item type, Pulse Book. Welcome to your welcome.\n",
      "210672_0jdhCdy6wPFMjRXl_0128                                 carbonic acid और sulphuric acid structures बनाना                                               Carbonic Acid or Sulfuric Acid Structures Carbonic Acid or Sulfuric Acid Structures\\n\\nThe original sentence is already in English, so there is no need to refine it. However, if you want to improve fluency and grammar, you can use the following sentence:\\n\\nCarbonic acid or sulfuric acid structures are commonly used in the production of various chemicals and materials.\\n\\nThis sentence is more fluent and grammatically correct than the original sentence. It uses the correct subject-verb agreement and includes appropriate punctuation.\n",
      "210672_0jdhCdy6wPFMjRXl_0053                                               अब sulphuric acid structure बनायें                                                                 פرو strawberry as a....                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              פור\n",
      "210672_0jdhCdy6wPFMjRXl_0116                           अब monocyclic compound को बाईcyclic compound में बदलें                           now, monocyclic compound co-bicyclic compound may be the case                                                                                                                                                                                                                                                                                                                                                                                                             नहीं, एक बार, एक बार, एक बार, एक बार, एक बार, एक बार, एक बार, एक बार, एक बार, एक बार, एक बार, एक बा�\n"
     ]
    }
   ],
   "source": [
    "# View random samples\n",
    "sample_df = df[[\"utt_id\", \"reference\", \"whisper_output\", \"llm_refined\"]].sample(5)\n",
    "print(sample_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05040a2",
   "metadata": {},
   "source": [
    "# LLM refinement v1b (Qwen, slightly different prompt text)\n",
    "    What it does:\n",
    "        - Reloads Whisper CSV, drops NaNs.\n",
    "        - Similar “preserve code-switching” prompt; generates “Improved:” style output and strips the marker.\n",
    "        - Saves → outputs_noffmpeg/llm_refined_transcripts.csv.   - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ee2870-374c-4eec-84e6-5d7a8f18a605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved refined output to: outputs_noffmpeg/llm_refined_transcripts.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "csv_path = \"outputs_noffmpeg/whisper_transcriptions_noffmpeg.csv\"\n",
    "output_path = \"outputs_noffmpeg/llm_refined_transcripts.csv\"\n",
    "model_name = \"Qwen/Qwen1.5-1.8B\"  # You can change this to other HF LLMs\n",
    "# ------------------------------------------\n",
    "\n",
    "# ✅ Load CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df.dropna(subset=[\"whisper_output\"])  # make sure no NaN\n",
    "\n",
    "# ✅ Load HuggingFace LLM with GPU support\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "# ✅ Controlled Prompt Function (no hallucinations)\n",
    "def refine_text(raw):\n",
    "    prompt = f\"\"\"Refine this Hindi-English sentence. Improve grammar and fluency while preserving code-switching points.\n",
    "\n",
    "Sentence: {raw}\n",
    "Improved:\"\"\"\n",
    "    output = llm(prompt, num_return_sequences=1)[0][\"generated_text\"]\n",
    "    return output.split(\"Improved:\")[-1].strip() if \"Improved:\" in output else output.strip()\n",
    "\n",
    "# ✅ Apply LLM refinement\n",
    "df[\"llm_refined\"] = df[\"whisper_output\"].apply(refine_text)\n",
    "\n",
    "# ✅ Save refined file\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"✅ Saved refined output to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6e6223",
   "metadata": {},
   "source": [
    "# Evaluate (WER/CER) for v1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "715b7ab8-3ed2-49cc-872f-02f0b2bc44a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Results:\n",
      "\n",
      "         Model    WER (%)    CER (%)\n",
      "Whisper Output 105.408426  84.201110\n",
      "   LLM Refined 193.219162 178.189485\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "# Load output\n",
    "df = pd.read_csv(output_path)\n",
    "df = df.dropna(subset=[\"reference\", \"whisper_output\", \"llm_refined\"])\n",
    "\n",
    "# Optional: Filter clearly broken whisper outputs\n",
    "df = df[df[\"whisper_output\"].str.count(r\"\\w+\") > 3]\n",
    "\n",
    "# Evaluate\n",
    "wer_whisper_scores = []\n",
    "wer_llm_scores = []\n",
    "cer_whisper_scores = []\n",
    "cer_llm_scores = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    wer_whisper_scores.append(wer(row[\"reference\"], row[\"whisper_output\"]))\n",
    "    wer_llm_scores.append(wer(row[\"reference\"], row[\"llm_refined\"]))\n",
    "    cer_whisper_scores.append(cer(row[\"reference\"], row[\"whisper_output\"]))\n",
    "    cer_llm_scores.append(cer(row[\"reference\"], row[\"llm_refined\"]))\n",
    "\n",
    "# Results Table\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Whisper Output\", \"LLM Refined\"],\n",
    "    \"WER (%)\": [\n",
    "        sum(wer_whisper_scores)/len(wer_whisper_scores)*100,\n",
    "        sum(wer_llm_scores)/len(wer_llm_scores)*100\n",
    "    ],\n",
    "    \"CER (%)\": [\n",
    "        sum(cer_whisper_scores)/len(cer_whisper_scores)*100,\n",
    "        sum(cer_llm_scores)/len(cer_llm_scores)*100\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display\n",
    "print(\"\\n📊 Evaluation Results:\\n\")\n",
    "print(results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da1037e",
   "metadata": {},
   "source": [
    "# “Clean” Qwen refinement (manual generate + decoding)\n",
    "    What it does:\n",
    "        - Loads Qwen tokenizer/model directly (AutoModelForCausalLM), filters short utterances.\n",
    "        - Builds the same style prompt, uses .generate() and careful decoding to strip the prompt.\n",
    "        - Saves → outputs_noffmpeg/llm_refined_transcripts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af37e4a0-3867-403c-b81c-74251b3009f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean LLM refinement done and saved.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"outputs_noffmpeg/whisper_transcriptions_noffmpeg.csv\")\n",
    "df = df.dropna(subset=[\"whisper_output\"])\n",
    "df = df[df[\"whisper_output\"].str.count(r\"\\w+\") > 3]  # Filter low content\n",
    "\n",
    "# Load model\n",
    "model_name = \"Qwen/Qwen1.5-1.8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Strict refinement (no hallucination)\n",
    "def refine_text(prompt_input):\n",
    "    prompt = f\"\"\"Refine this Hindi-English sentence. Keep code-switching unchanged.\n",
    "                Sentence: {prompt_input}\n",
    "                Refined:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the part after \"Refined:\"\n",
    "    if \"Refined:\" in decoded:\n",
    "        return decoded.split(\"Refined:\")[-1].strip()\n",
    "    else:\n",
    "        return decoded.replace(prompt, \"\").strip()\n",
    "\n",
    "# Apply clean refinement\n",
    "df[\"llm_refined\"] = df[\"whisper_output\"].apply(refine_text)\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"outputs_noffmpeg/llm_refined_transcripts.csv\", index=False)\n",
    "print(\"✅ Clean LLM refinement done and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b5289",
   "metadata": {},
   "source": [
    "# Evaluate (WER/CER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d63a060a-2dfc-4268-9247-b52c25c561bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Results:\n",
      "\n",
      "         Model    WER (%)    CER (%)\n",
      "Whisper Output 105.408426  84.201110\n",
      "   LLM Refined 193.219162 178.189485\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "# Load output\n",
    "df = pd.read_csv(output_path)\n",
    "df = df.dropna(subset=[\"reference\", \"whisper_output\", \"llm_refined\"])\n",
    "\n",
    "# Optional: Filter clearly broken whisper outputs\n",
    "df = df[df[\"whisper_output\"].str.count(r\"\\w+\") > 3]\n",
    "\n",
    "# Evaluate\n",
    "wer_whisper_scores = []\n",
    "wer_llm_scores = []\n",
    "cer_whisper_scores = []\n",
    "cer_llm_scores = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    wer_whisper_scores.append(wer(row[\"reference\"], row[\"whisper_output\"]))\n",
    "    wer_llm_scores.append(wer(row[\"reference\"], row[\"llm_refined\"]))\n",
    "    cer_whisper_scores.append(cer(row[\"reference\"], row[\"whisper_output\"]))\n",
    "    cer_llm_scores.append(cer(row[\"reference\"], row[\"llm_refined\"]))\n",
    "\n",
    "# Results Table\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Whisper Output\", \"LLM Refined\"],\n",
    "    \"WER (%)\": [\n",
    "        sum(wer_whisper_scores)/len(wer_whisper_scores)*100,\n",
    "        sum(wer_llm_scores)/len(wer_llm_scores)*100\n",
    "    ],\n",
    "    \"CER (%)\": [\n",
    "        sum(cer_whisper_scores)/len(cer_whisper_scores)*100,\n",
    "        sum(cer_llm_scores)/len(cer_llm_scores)*100\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display\n",
    "print(\"\\n📊 Evaluation Results:\\n\")\n",
    "print(results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "519f8f0e-5594-420e-b016-4fdbe439d76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting resampy\n",
      "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from resampy) (1.25.2)\n",
      "Requirement already satisfied: numba>=0.53 in ./.local/lib/python3.11/site-packages (from resampy) (0.61.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.local/lib/python3.11/site-packages (from numba>=0.53->resampy) (0.44.0)\n",
      "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: resampy\n",
      "Successfully installed resampy-0.4.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install resampy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88346a54",
   "metadata": {},
   "source": [
    "# STEP 1 (Whisper “medium”) without ffmpeg\n",
    "\n",
    "Uses soundfile + resampy to read/ensure 16 kHz mono; transcribes segments with whisper model \"medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1553b3-dea1-41bb-acac-b67e9fc2ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Whisper transcription complete.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 1: Transcribe using Whisper (medium) without FFmpeg\n",
    "# ---------------------------\n",
    "import whisper\n",
    "import os\n",
    "import soundfile as sf\n",
    "import resampy\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"train_split/transcripts\")\n",
    "OUTPUT_DIR = Path(\"outputs_medium\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "# Load metadata\n",
    "segments = pd.read_csv(DATA_DIR / \"segments\", sep='\\s+', names=[\"utt_id\", \"wav_id\", \"start\", \"end\"])\n",
    "\n",
    "# Fix for WAV file loading based on actual disk structure\n",
    "wav_scp_raw = pd.read_csv(DATA_DIR / \"wav.scp\", sep='\\s+', header=None)\n",
    "wav_scp = {\n",
    "    row[0]: str(DATA_DIR / row[1])  # use raw file path directly without adding .wav\n",
    "    for row in wav_scp_raw.values\n",
    "}\n",
    "\n",
    "# Parse 'text' file manually to handle spaces in transcript\n",
    "utt2text = {}\n",
    "with open(DATA_DIR / \"text\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            utt2text[parts[0]] = parts[1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop over segments\n",
    "for seg in segments.itertuples():\n",
    "    wav_path = wav_scp.get(seg.wav_id)\n",
    "    if not wav_path:\n",
    "        continue\n",
    "\n",
    "    full_path = Path(wav_path)\n",
    "    if not full_path.exists():\n",
    "        print(f\"❌ File not found: {full_path}\")\n",
    "        continue\n",
    "\n",
    "    y, sr = sf.read(full_path)\n",
    "    start_sample = int(seg.start * sr)\n",
    "    end_sample = int(seg.end * sr)\n",
    "    y_seg = y[start_sample:end_sample]\n",
    "\n",
    "    # Convert stereo to mono if needed\n",
    "    if len(y_seg.shape) > 1:\n",
    "        y_seg = np.mean(y_seg, axis=1)\n",
    "\n",
    "    # Ensure sample rate is 16kHz as required by Whisper\n",
    "    if sr != 16000:\n",
    "        y_seg = resampy.resample(y_seg, sr, 16000)\n",
    "        sr = 16000\n",
    "\n",
    "    # Convert to float32 to avoid dtype mismatch\n",
    "    y_seg = y_seg.astype(np.float32)\n",
    "\n",
    "    # Transcribe directly from numpy audio\n",
    "    result = model.transcribe(y_seg, language=\"hi\", fp16=False)\n",
    "    results.append({\n",
    "        \"utt_id\": seg.utt_id,\n",
    "        \"reference\": utt2text.get(seg.utt_id, \"\"),\n",
    "        \"whisper_output\": result['text'].strip()\n",
    "    })\n",
    "\n",
    "# Save transcriptions\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(OUTPUT_DIR / \"whisper_transcriptions_medium.csv\", index=False)\n",
    "print(\"\\u2705 Whisper transcription complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8fabc9",
   "metadata": {},
   "source": [
    "# STEP 2 (LLM post-processing, Qwen)\n",
    "\n",
    "Loads the medium CSV; builds preserve-code-switching prompt; runs Qwen 1.5-1.8B; saves → outputs_medium/llm_refined_transcripts.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bbc39f2-35c7-49b9-af71-09fd004d2782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Running LLM refinement (preserving code-switching)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM refinement completed.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 2: LLM Post-processing (preserve code-switching)\n",
    "# ---------------------------\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-1.8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Reload CSV\n",
    "df = pd.read_csv(OUTPUT_DIR / \"whisper_transcriptions_medium.csv\")\n",
    "df = df.dropna(subset=[\"whisper_output\"])\n",
    "df = df[df[\"whisper_output\"].str.count(r\"\\w+\") > 3]\n",
    "\n",
    "# Refine preserving code-switching\n",
    "\n",
    "def refine_text(prompt_input):\n",
    "    prompt = (\n",
    "        \"The following sentence contains Hindi-English code-switching. \"\n",
    "        \"Refine it by preserving the structure and language switching as it is. \"\n",
    "        \"Do not correct grammar or translate to one language. Keep Hindi words in Devanagari script. \"\n",
    "        \"Improve intelligibility only where needed, but avoid altering meaning.\\n\\n\"\n",
    "        f\"Sentence: {prompt_input}\\n\"\n",
    "        \"Refined:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"Refined:\")[-1].strip()\n",
    "\n",
    "# Apply\n",
    "print(\"\\u23F3 Running LLM refinement (preserving code-switching)...\")\n",
    "df[\"llm_refined\"] = df[\"whisper_output\"].apply(refine_text)\n",
    "df.to_csv(OUTPUT_DIR / \"llm_refined_transcripts.csv\", index=False)\n",
    "print(\"\\u2705 LLM refinement completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22aea10",
   "metadata": {},
   "source": [
    "# STEP 3 (Evaluate medium + Qwen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc0b706a-e5ba-4600-9390-7c2469e02313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Results:\n",
      "\n",
      "         Model    WER (%)     CER (%)\n",
      "Whisper Output  84.763362   74.156412\n",
      "   LLM Refined 817.949885 1139.750696\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 3: Evaluation\n",
    "# ---------------------------\n",
    "from jiwer import wer, cer\n",
    "\n",
    "# Load for evaluation\n",
    "df = pd.read_csv(OUTPUT_DIR / \"llm_refined_transcripts.csv\")\n",
    "df = df.dropna(subset=[\"reference\", \"whisper_output\", \"llm_refined\"])\n",
    "\n",
    "# Evaluation\n",
    "wer_whisper = [wer(r, h) for r, h in zip(df.reference, df.whisper_output)]\n",
    "wer_llm = [wer(r, h) for r, h in zip(df.reference, df.llm_refined)]\n",
    "cer_whisper = [cer(r, h) for r, h in zip(df.reference, df.whisper_output)]\n",
    "cer_llm = [cer(r, h) for r, h in zip(df.reference, df.llm_refined)]\n",
    "\n",
    "# Summary table\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Whisper Output\", \"LLM Refined\"],\n",
    "    \"WER (%)\": [sum(wer_whisper)/len(wer_whisper)*100, sum(wer_llm)/len(wer_llm)*100],\n",
    "    \"CER (%)\": [sum(cer_whisper)/len(cer_whisper)*100, sum(cer_llm)/len(cer_llm)*100]\n",
    "})\n",
    "\n",
    "print(\"\\n\\U0001F4CA Evaluation Results:\\n\")\n",
    "print(results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02dce6",
   "metadata": {},
   "source": [
    "# Combined pipeline (Whisper medium → Qwen → Eval) in one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2252537-fb39-4bb2-aaf8-c0b09bd49440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Whisper transcription complete.\n",
      "⏳ Running LLM refinement (preserving code-switching)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM refinement completed.\n",
      "\n",
      "📊 Evaluation Results:\n",
      "\n",
      "         Model    WER (%)    CER (%)\n",
      "Whisper Output  85.921188  76.559156\n",
      "   LLM Refined 725.098964 969.756419\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 1: Transcribe using Whisper (medium) without FFmpeg\n",
    "# ---------------------------\n",
    "import whisper\n",
    "import os\n",
    "import soundfile as sf\n",
    "import resampy\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"train_split/transcripts\")\n",
    "OUTPUT_DIR = Path(\"outputs_medium\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "# Load metadata\n",
    "segments = pd.read_csv(DATA_DIR / \"segments\", sep='\\s+', names=[\"utt_id\", \"wav_id\", \"start\", \"end\"])\n",
    "\n",
    "# Fix for WAV file loading based on actual disk structure\n",
    "wav_scp_raw = pd.read_csv(DATA_DIR / \"wav.scp\", sep='\\s+', header=None)\n",
    "wav_scp = {\n",
    "    row[0]: str(DATA_DIR / row[1])\n",
    "    for row in wav_scp_raw.values\n",
    "}\n",
    "\n",
    "# Parse 'text' file manually to handle spaces in transcript\n",
    "utt2text = {}\n",
    "with open(DATA_DIR / \"text\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            utt2text[parts[0]] = parts[1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop over segments\n",
    "for seg in segments.itertuples():\n",
    "    wav_path = wav_scp.get(seg.wav_id)\n",
    "    if not wav_path:\n",
    "        continue\n",
    "\n",
    "    full_path = Path(wav_path)\n",
    "    if not full_path.exists():\n",
    "        print(f\"❌ File not found: {full_path}\")\n",
    "        continue\n",
    "\n",
    "    y, sr = sf.read(full_path)\n",
    "    start_sample = int(seg.start * sr)\n",
    "    end_sample = int(seg.end * sr)\n",
    "    y_seg = y[start_sample:end_sample]\n",
    "\n",
    "    # Convert stereo to mono if needed\n",
    "    if len(y_seg.shape) > 1:\n",
    "        y_seg = np.mean(y_seg, axis=1)\n",
    "\n",
    "    # Ensure sample rate is 16kHz as required by Whisper\n",
    "    if sr != 16000:\n",
    "        y_seg = resampy.resample(y_seg, sr, 16000)\n",
    "        sr = 16000\n",
    "\n",
    "    # Convert to float32 to avoid dtype mismatch\n",
    "    y_seg = y_seg.astype(np.float32)\n",
    "\n",
    "    # Transcribe directly from numpy audio\n",
    "    result = model.transcribe(y_seg, language=\"hi\", fp16=False)\n",
    "    results.append({\n",
    "        \"utt_id\": seg.utt_id,\n",
    "        \"reference\": utt2text.get(seg.utt_id, \"\"),\n",
    "        \"whisper_output\": result['text'].strip()\n",
    "    })\n",
    "\n",
    "# Save transcriptions\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(OUTPUT_DIR / \"whisper_transcriptions_medium.csv\", index=False)\n",
    "print(\"\\u2705 Whisper transcription complete.\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# STEP 2: LLM Post-processing (preserve code-switching)\n",
    "# ---------------------------\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen1.5-1.8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Reload CSV\n",
    "df = pd.read_csv(OUTPUT_DIR / \"whisper_transcriptions_medium.csv\")\n",
    "df = df.dropna(subset=[\"whisper_output\"])\n",
    "df = df[df[\"whisper_output\"].str.count(r\"\\w+\") > 3]\n",
    "\n",
    "# Refine preserving code-switching\n",
    "\n",
    "def refine_text(prompt_input):\n",
    "    prompt = (\n",
    "        \"Below is a code-switched Hindi-English sentence. Preserve the exact content and language switching style.\\n\"\n",
    "        \"Do NOT correct grammar. Do NOT translate. Only enhance understandability minimally.\\n\"\n",
    "        \"Keep Hindi words in Devanagari script.\\n\"\n",
    "        f\"Sentence: {prompt_input}\\n\"\n",
    "        \"Output:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"Output:\")[-1].strip()\n",
    "\n",
    "# Apply\n",
    "print(\"\\u23F3 Running LLM refinement (preserving code-switching)...\")\n",
    "df[\"llm_refined\"] = df[\"whisper_output\"].apply(refine_text)\n",
    "df.to_csv(OUTPUT_DIR / \"llm_refined_transcripts.csv\", index=False)\n",
    "print(\"\\u2705 LLM refinement completed.\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# STEP 3: Evaluation\n",
    "# ---------------------------\n",
    "from jiwer import wer, cer\n",
    "\n",
    "# Load for evaluation\n",
    "df = pd.read_csv(OUTPUT_DIR / \"llm_refined_transcripts.csv\")\n",
    "df = df.dropna(subset=[\"reference\", \"whisper_output\", \"llm_refined\"])\n",
    "\n",
    "# Evaluation\n",
    "wer_whisper = [wer(r, h) for r, h in zip(df.reference, df.whisper_output)]\n",
    "wer_llm = [wer(r, h) for r, h in zip(df.reference, df.llm_refined)]\n",
    "cer_whisper = [cer(r, h) for r, h in zip(df.reference, df.whisper_output)]\n",
    "cer_llm = [cer(r, h) for r, h in zip(df.reference, df.llm_refined)]\n",
    "\n",
    "# Summary table\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Whisper Output\", \"LLM Refined\"],\n",
    "    \"WER (%)\": [sum(wer_whisper)/len(wer_whisper)*100, sum(wer_llm)/len(wer_llm)*100],\n",
    "    \"CER (%)\": [sum(cer_whisper)/len(cer_whisper)*100, sum(cer_llm)/len(cer_llm)*100]\n",
    "})\n",
    "\n",
    "print(\"\\n\\U0001F4CA Evaluation Results:\\n\")\n",
    "print(results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c5d1f",
   "metadata": {},
   "source": [
    "# Combined pipeline (Whisper medium → FLAN-T5 → Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce24abc9-7f13-4f33-8992-5fbeccfed195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Whisper transcription complete.\n",
      "⏳ Running LLM refinement (FLAN-T5, preserving code-switching)...\n",
      "✅ LLM refinement completed.\n",
      "\n",
      "📊 Evaluation Results:\n",
      "\n",
      "         Model    WER (%)    CER (%)\n",
      "Whisper Output  78.701599  71.404357\n",
      "   LLM Refined 123.244692 113.442642\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 1: Transcribe using Whisper (medium) without FFmpeg\n",
    "# ---------------------------\n",
    "import whisper\n",
    "import os\n",
    "import soundfile as sf\n",
    "import resampy\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"train_split/transcripts\")\n",
    "OUTPUT_DIR = Path(\"outputs_medium\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "# Load metadata\n",
    "segments = pd.read_csv(DATA_DIR / \"segments\", sep='\\s+', names=[\"utt_id\", \"wav_id\", \"start\", \"end\"])\n",
    "\n",
    "# Fix for WAV file loading based on actual disk structure\n",
    "wav_scp_raw = pd.read_csv(DATA_DIR / \"wav.scp\", sep='\\s+', header=None)\n",
    "wav_scp = {\n",
    "    row[0]: str(DATA_DIR / row[1])\n",
    "    for row in wav_scp_raw.values\n",
    "}\n",
    "\n",
    "# Parse 'text' file manually to handle spaces in transcript\n",
    "utt2text = {}\n",
    "with open(DATA_DIR / \"text\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            utt2text[parts[0]] = parts[1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop over segments\n",
    "for seg in segments.itertuples():\n",
    "    wav_path = wav_scp.get(seg.wav_id)\n",
    "    if not wav_path:\n",
    "        continue\n",
    "\n",
    "    full_path = Path(wav_path)\n",
    "    if not full_path.exists():\n",
    "        print(f\"❌ File not found: {full_path}\")\n",
    "        continue\n",
    "\n",
    "    y, sr = sf.read(full_path)\n",
    "    start_sample = int(seg.start * sr)\n",
    "    end_sample = int(seg.end * sr)\n",
    "    y_seg = y[start_sample:end_sample]\n",
    "\n",
    "    # Convert stereo to mono if needed\n",
    "    if len(y_seg.shape) > 1:\n",
    "        y_seg = np.mean(y_seg, axis=1)\n",
    "\n",
    "    # Ensure sample rate is 16kHz as required by Whisper\n",
    "    if sr != 16000:\n",
    "        y_seg = resampy.resample(y_seg, sr, 16000)\n",
    "        sr = 16000\n",
    "\n",
    "    # Convert to float32 to avoid dtype mismatch\n",
    "    y_seg = y_seg.astype(np.float32)\n",
    "\n",
    "    # Transcribe directly from numpy audio\n",
    "    result = model.transcribe(y_seg, language=\"hi\", fp16=False)\n",
    "    results.append({\n",
    "        \"utt_id\": seg.utt_id,\n",
    "        \"reference\": utt2text.get(seg.utt_id, \"\"),\n",
    "        \"whisper_output\": result['text'].strip()\n",
    "    })\n",
    "\n",
    "# Save transcriptions\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(OUTPUT_DIR / \"whisper_transcriptions_medium.csv\", index=False)\n",
    "print(\"\\u2705 Whisper transcription complete.\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# STEP 2: LLM Post-processing (preserve code-switching) using FLAN-T5\n",
    "# ---------------------------\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reload CSV\n",
    "df = pd.read_csv(OUTPUT_DIR / \"whisper_transcriptions_medium.csv\")\n",
    "df = df.dropna(subset=[\"whisper_output\"])\n",
    "df = df[df[\"whisper_output\"].str.count(r\"\\w+\") > 3]\n",
    "\n",
    "# Refine preserving code-switching without translating or correcting grammar\n",
    "def refine_text(prompt_input):\n",
    "    prompt = (\n",
    "        \"Keep the Hindi-English code-switching as it is. Do not translate. \"\n",
    "        \"Do not correct grammar. Just clean any hallucinations or repetition and ensure readability.\\n\"\n",
    "        f\"Input: {prompt_input}\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"\\u23F3 Running LLM refinement (FLAN-T5, preserving code-switching)...\")\n",
    "df[\"llm_refined\"] = df[\"whisper_output\"].apply(refine_text)\n",
    "df.to_csv(OUTPUT_DIR / \"llm_refined_transcripts.csv\", index=False)\n",
    "print(\"\\u2705 LLM refinement completed.\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# STEP 3: Evaluation\n",
    "# ---------------------------\n",
    "from jiwer import wer, cer\n",
    "\n",
    "# Load for evaluation\n",
    "df = pd.read_csv(OUTPUT_DIR / \"llm_refined_transcripts.csv\")\n",
    "df = df.dropna(subset=[\"reference\", \"whisper_output\", \"llm_refined\"])\n",
    "\n",
    "# Evaluation\n",
    "wer_whisper = [wer(r, h) for r, h in zip(df.reference, df.whisper_output)]\n",
    "wer_llm = [wer(r, h) for r, h in zip(df.reference, df.llm_refined)]\n",
    "cer_whisper = [cer(r, h) for r, h in zip(df.reference, df.whisper_output)]\n",
    "cer_llm = [cer(r, h) for r, h in zip(df.reference, df.llm_refined)]\n",
    "\n",
    "# Summary table\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Whisper Output\", \"LLM Refined\"],\n",
    "    \"WER (%)\": [sum(wer_whisper)/len(wer_whisper)*100, sum(wer_llm)/len(wer_llm)*100],\n",
    "    \"CER (%)\": [sum(cer_whisper)/len(cer_whisper)*100, sum(cer_llm)/len(cer_llm)*100]\n",
    "})\n",
    "\n",
    "print(\"\\n\\U0001F4CA Evaluation Results:\\n\")\n",
    "print(results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62fa490-441c-4726-b728-2b77d5f6160c",
   "metadata": {},
   "source": [
    "# Google FLAN-T5 with few-shot examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7923077f-03bd-4bca-84e8-82468b78c4f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Running LLM refinement (FLAN-T5 with few-shot)...\n",
      "✅ LLM refinement completed.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 2: LLM Post-processing (preserve code-switching) using FLAN-T5 with few-shot examples\n",
    "# ---------------------------\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reload transcriptions\n",
    "df = pd.read_csv(OUTPUT_DIR / \"whisper_transcriptions_medium.csv\")\n",
    "df = df.dropna(subset=[\"whisper_output\"])\n",
    "df = df[df[\"whisper_output\"].str.count(r\"\\w+\") > 3]\n",
    "\n",
    "# Few-shot prompt-based code-switching-aware cleaner\n",
    "def refine_text(whisper_output):\n",
    "    prompt = (\n",
    "        \"You are a bilingual speech-to-text cleaner for Hindi-English code-switched transcripts.\\n\"\n",
    "        \"Your goal is to ONLY remove hallucinations or repeated/fake words from the transcript.\\n\"\n",
    "        \"DO NOT paraphrase. DO NOT translate. DO NOT correct grammar. DO NOT add new words.\\n\"\n",
    "        \"Keep English and Hindi words as-is.\\n\\n\"\n",
    "        \"Example 1:\\n\"\n",
    "        \"Input: spoken tutorial project talktoa teacher project पर आधारित है\\n\"\n",
    "        \"Output: spoken tutorial project talk to a teacher project पर आधारित है\\n\\n\"\n",
    "        \"Example 2:\\n\"\n",
    "        \"Input: अब monocyclic compound co-bicyclic compound may be the case\\n\"\n",
    "        \"Output: अब monocyclic compound को bicyclic compound may be the case\\n\\n\"\n",
    "        \"Example 3:\\n\"\n",
    "        \"Input: carbonic acid or sulphuric acid structures carbonic acid or sulphuric acid structures\\n\"\n",
    "        \"Output: carbonic acid or sulphuric acid structures\\n\\n\"\n",
    "        f\"Now fix this:\\nInput: {whisper_output}\\nOutput:\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\u23F3 Running LLM refinement (FLAN-T5 with few-shot)...\")\n",
    "df[\"llm_refined\"] = df[\"whisper_output\"].apply(refine_text)\n",
    "df.to_csv(OUTPUT_DIR / \"llm_refined_transcripts.csv\", index=False)\n",
    "print(\"\\u2705 LLM refinement completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9cec8",
   "metadata": {},
   "source": [
    "Evaluate FLAN-T5 few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba0720ed-48cb-4e96-afaa-cd2a85cf6737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Results:\n",
      "\n",
      "         Model   WER (%)   CER (%)\n",
      "Whisper Output 95.176844 93.352028\n",
      "   LLM Refined 97.436419 98.743169\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 3: Evaluation\n",
    "# ---------------------------\n",
    "from jiwer import wer, cer\n",
    "\n",
    "# Load for evaluation\n",
    "df = pd.read_csv(OUTPUT_DIR / \"llm_refined_transcripts.csv\")\n",
    "df = df.dropna(subset=[\"reference\", \"whisper_output\", \"llm_refined\"])\n",
    "\n",
    "# Evaluation\n",
    "wer_whisper = [wer(r, h) for r, h in zip(df.reference, df.whisper_output)]\n",
    "wer_llm = [wer(r, h) for r, h in zip(df.reference, df.llm_refined)]\n",
    "cer_whisper = [cer(r, h) for r, h in zip(df.reference, df.whisper_output)]\n",
    "cer_llm = [cer(r, h) for r, h in zip(df.reference, df.llm_refined)]\n",
    "\n",
    "# Summary table\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Whisper Output\", \"LLM Refined\"],\n",
    "    \"WER (%)\": [sum(wer_whisper)/len(wer_whisper)*100, sum(wer_llm)/len(wer_llm)*100],\n",
    "    \"CER (%)\": [sum(cer_whisper)/len(cer_whisper)*100, sum(cer_llm)/len(cer_llm)*100]\n",
    "})\n",
    "\n",
    "print(\"\\n\\U0001F4CA Evaluation Results:\\n\")\n",
    "print(results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91dbe36-80dd-4b7b-874b-3087f4b1f3c7",
   "metadata": {},
   "source": [
    "# Tagging Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07c40ef6-ce96-4a4a-8db1-419dafe55105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pybind11>=2.2 (from fasttext)\n",
      "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (69.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.25.2)\n",
      "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313499 sha256=b5f9c3552371c295252549cf7d934d12587f2b6f46671898abb4f945bbee9039\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1f3778-a8ee-426b-81b6-1d33da7d9af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading FastText language model...\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 1: Whisper Transcription\n",
    "# ---------------------------\n",
    "import whisper\n",
    "import os\n",
    "import soundfile as sf\n",
    "import resampy\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"train_split/transcripts\")\n",
    "OUTPUT_DIR = Path(\"outputs_tagged\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "# Load FastText language identification model (download if not present)\n",
    "if not Path(\"lid.176.bin\").exists():\n",
    "    import urllib.request\n",
    "    print(\"Downloading FastText language model...\")\n",
    "    urllib.request.urlretrieve(\"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\", \"lid.176.bin\")\n",
    "\n",
    "lang_model = fasttext.load_model(\"lid.176.bin\")\n",
    "\n",
    "# Load metadata\n",
    "segments = pd.read_csv(DATA_DIR / \"segments\", sep='\\s+', names=[\"utt_id\", \"wav_id\", \"start\", \"end\"])\n",
    "wav_scp_raw = pd.read_csv(DATA_DIR / \"wav.scp\", sep='\\s+', header=None)\n",
    "wav_scp = {row[0]: str(DATA_DIR / row[1]) for row in wav_scp_raw.values}\n",
    "\n",
    "utt2text = {}\n",
    "with open(DATA_DIR / \"text\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            utt2text[parts[0]] = parts[1]\n",
    "\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4814749",
   "metadata": {},
   "source": [
    "# Word-level language tagging\n",
    "\n",
    "What it does:\n",
    "- For each Whisper hypothesis, predicts each token’s language with fastText; prepends <HI> / <EN> tags and joins back.\n",
    "- Saves → outputs_medium/whisper_tagged_transcriptions.csv with columns: utt_id, reference, whisper_output, tagged_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fcec596-8f51-40e5-8596-3c19b6ef9048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Whisper transcription and tagging complete.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 2: Transcription and Word-Level Language Tagging\n",
    "# ---------------------------\n",
    "def tag_languages(text):\n",
    "    words = text.strip().split()\n",
    "    tagged = []\n",
    "    for word in words:\n",
    "        lang = lang_model.predict(word.replace(\"।\", \"\").replace(\".\", \"\"))[0][0].replace(\"__label__\", \"\")\n",
    "        tag = \"<HI>\" if lang == \"hi\" else \"<EN>\"\n",
    "        tagged.append(f\"{tag} {word}\")\n",
    "    return \" \".join(tagged)\n",
    "\n",
    "for seg in segments.itertuples():\n",
    "    wav_path = wav_scp.get(seg.wav_id)\n",
    "    if not wav_path:\n",
    "        continue\n",
    "\n",
    "    full_path = Path(wav_path)\n",
    "    if not full_path.exists():\n",
    "        print(f\"❌ File not found: {full_path}\")\n",
    "        continue\n",
    "\n",
    "    y, sr = sf.read(full_path)\n",
    "    start_sample = int(seg.start * sr)\n",
    "    end_sample = int(seg.end * sr)\n",
    "    y_seg = y[start_sample:end_sample]\n",
    "\n",
    "    if len(y_seg.shape) > 1:\n",
    "        y_seg = np.mean(y_seg, axis=1)\n",
    "\n",
    "    if sr != 16000:\n",
    "        y_seg = resampy.resample(y_seg, sr, 16000)\n",
    "        sr = 16000\n",
    "\n",
    "    y_seg = y_seg.astype(np.float32)\n",
    "\n",
    "    result = model.transcribe(y_seg, fp16=False)\n",
    "    output_text = result['text'].strip()\n",
    "    tagged_text = tag_languages(output_text)\n",
    "\n",
    "    results.append({\n",
    "        \"utt_id\": seg.utt_id,\n",
    "        \"reference\": utt2text.get(seg.utt_id, \"\"),\n",
    "        \"whisper_output\": output_text,\n",
    "        \"tagged_output\": tagged_text\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(OUTPUT_DIR / \"whisper_tagged_transcriptions.csv\", index=False)\n",
    "print(\"✅ Whisper transcription and tagging complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a7740",
   "metadata": {},
   "source": [
    "# LLM refinement with tags; FLAN-T5\n",
    "\n",
    "What it does:\n",
    "- Loads flan-t5-large, builds a prompt that says: “Given tagged text (<HI>, <EN>), improve readability while preserving tags/code-switch pattern; no translation or meaning change.”\n",
    "- Generates refined text per row; saves → outputs_medium/llm_refined_transcripts.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10a72f6-d9aa-43cb-98f7-e0e7c8b6612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Running LLM refinement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM refinement completed.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 3: LLM Refinement (Preserve Code-switching with Tags)\n",
    "# ---------------------------\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "print(\"⏳ Running LLM refinement...\")\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "llm = T5ForConditionalGeneration.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def refine_with_tags(tagged_text):\n",
    "    prompt = (\n",
    "        \"You are an expert at processing Hindi-English code-switched transcriptions. \"\n",
    "        \"Keep <HI> Hindi and <EN> English tags intact. Only remove repetition or hallucinated words. Do not correct grammar. Do not change languages.\\n\\n\"\n",
    "        f\"Input: {tagged_text}\\nOutput:\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(llm.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = llm.generate(**inputs, max_new_tokens=128)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "df[\"llm_refined\"] = df[\"tagged_output\"].apply(refine_with_tags)\n",
    "df.to_csv(OUTPUT_DIR / \"llm_refined_transcripts.csv\", index=False)\n",
    "print(\"✅ LLM refinement completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c1224f-b29f-474f-bad2-4701f4fe40f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Results:\n",
      "\n",
      "         Model    WER (%)    CER (%)\n",
      "Whisper Output  78.260012  69.075436\n",
      "   LLM Refined 136.169147 125.845370\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 4: Evaluation (WER/CER)\n",
    "# ---------------------------\n",
    "from jiwer import wer, cer\n",
    "\n",
    "df = pd.read_csv(OUTPUT_DIR / \"llm_refined_transcripts.csv\")\n",
    "df = df.dropna(subset=[\"reference\", \"whisper_output\", \"llm_refined\"])\n",
    "\n",
    "# Clean tags before evaluation\n",
    "def clean_tags(text):\n",
    "    return re.sub(r\"</?EN>|</?HI>\", \"\", text)\n",
    "\n",
    "wer_whisper = [wer(r, h) for r, h in zip(df.reference, df.whisper_output)]\n",
    "wer_llm = [wer(r, clean_tags(h)) for r, h in zip(df.reference, df.llm_refined)]\n",
    "cer_whisper = [cer(r, h) for r, h in zip(df.reference, df.whisper_output)]\n",
    "cer_llm = [cer(r, clean_tags(h)) for r, h in zip(df.reference, df.llm_refined)]\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Whisper Output\", \"LLM Refined\"],\n",
    "    \"WER (%)\": [sum(wer_whisper)/len(wer_whisper)*100, sum(wer_llm)/len(wer_llm)*100],\n",
    "    \"CER (%)\": [sum(cer_whisper)/len(cer_whisper)*100, sum(cer_llm)/len(cer_llm)*100]\n",
    "})\n",
    "\n",
    "print(\"\\n📊 Evaluation Results:\\n\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e80bb-11ce-4a7a-b96b-a676f63ea9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
