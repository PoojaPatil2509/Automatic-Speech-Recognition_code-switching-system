{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c5e011-8ea7-4a48-8be4-e6c735affe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the zipfile module \n",
    "from zipfile import ZipFile \n",
    "  \n",
    "# loading the temp.zip and creating a zip object \n",
    "with ZipFile('train_sample.zip', 'r') as zObject: \n",
    "  \n",
    "    # Extracting all the members of the zip  \n",
    "    # into a specific location. \n",
    "    zObject.extractall( \n",
    "        path=\"./\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d660f27-b54c-4064-92d3-6fb73ae089e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.45.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Collecting click>=8.1.8 (from jiwer)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: torch==2.4.1 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchaudio) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchaudio) (12.6.77)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.1->torchaudio) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.1->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, requests, rapidfuzz, dill, click, multiprocess, jiwer, datasets\n",
      "Successfully installed click-8.2.1 datasets-4.0.0 dill-0.3.8 jiwer-4.0.0 multiprocess-0.70.16 rapidfuzz-3.13.0 requests-2.32.4 xxhash-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets jiwer torchaudio sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e289759e-9f24-46f2-a637-b1031b4ab79c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.25.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: soundfile\n",
      "Successfully installed soundfile-0.13.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0bac92",
   "metadata": {},
   "source": [
    "# Full hybrid pipeline v1 (Whisper encoder + MoE + CTC) → evaluate → LLM few-shot refine (Mistral-7B)\n",
    "- Utilities for Kaldi-style inputs (wav.scp, segments, text) and robust audio loading (torchaudio → soundfile fallback).\n",
    "- Dataset + collate: packs Whisper processor features and label tensors.\n",
    "- Model: WhisperMoEModel (Whisper encoder → small Mixture-of-Experts block → projection to vocab) trained with CTC.\n",
    "- Training: Adam, prints per-epoch train/val loss; saves best.pth on improvement.\n",
    "- Greedy CTC decode → WER/CER calculation.\n",
    "- LLM refinement: loads mistralai/Mistral-7B-Instruct-v0.3 with a few-shot prompt; batches generation; cleans output; re-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6a2c51-7890-4c5a-9a12-95e92ecdc079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75  Train Loss: 299.723  Val Loss: 221.473\n",
      "Saved new best model\n",
      "Epoch 2/75  Train Loss: 112.520  Val Loss: 77.578\n",
      "Saved new best model\n",
      "Epoch 3/75  Train Loss: 54.269  Val Loss: 57.761\n",
      "Saved new best model\n",
      "Epoch 4/75  Train Loss: 13.481  Val Loss: 5.073\n",
      "Saved new best model\n",
      "Epoch 5/75  Train Loss: 4.806  Val Loss: 4.987\n",
      "Saved new best model\n",
      "Epoch 6/75  Train Loss: 4.559  Val Loss: 4.612\n",
      "Saved new best model\n",
      "Epoch 7/75  Train Loss: 4.456  Val Loss: 4.524\n",
      "Saved new best model\n",
      "Epoch 8/75  Train Loss: 4.363  Val Loss: 4.417\n",
      "Saved new best model\n",
      "Epoch 9/75  Train Loss: 4.287  Val Loss: 4.419\n",
      "Epoch 10/75  Train Loss: 4.214  Val Loss: 4.263\n",
      "Saved new best model\n",
      "Epoch 11/75  Train Loss: 4.175  Val Loss: 4.304\n",
      "Epoch 12/75  Train Loss: 4.119  Val Loss: 4.093\n",
      "Saved new best model\n",
      "Epoch 13/75  Train Loss: 4.026  Val Loss: 4.007\n",
      "Saved new best model\n",
      "Epoch 14/75  Train Loss: 3.935  Val Loss: 3.912\n",
      "Saved new best model\n",
      "Epoch 15/75  Train Loss: 3.866  Val Loss: 3.812\n",
      "Saved new best model\n",
      "Epoch 16/75  Train Loss: 3.800  Val Loss: 3.833\n",
      "Epoch 17/75  Train Loss: 3.723  Val Loss: 3.668\n",
      "Saved new best model\n",
      "Epoch 18/75  Train Loss: 3.623  Val Loss: 3.527\n",
      "Saved new best model\n",
      "Epoch 19/75  Train Loss: 3.566  Val Loss: 3.453\n",
      "Saved new best model\n",
      "Epoch 20/75  Train Loss: 3.497  Val Loss: 3.377\n",
      "Saved new best model\n",
      "Epoch 21/75  Train Loss: 3.431  Val Loss: 3.256\n",
      "Saved new best model\n",
      "Epoch 22/75  Train Loss: 3.341  Val Loss: 3.168\n",
      "Saved new best model\n",
      "Epoch 23/75  Train Loss: 3.270  Val Loss: 3.184\n",
      "Epoch 24/75  Train Loss: 3.236  Val Loss: 3.041\n",
      "Saved new best model\n",
      "Epoch 25/75  Train Loss: 3.093  Val Loss: 2.878\n",
      "Saved new best model\n",
      "Epoch 26/75  Train Loss: 3.007  Val Loss: 2.744\n",
      "Saved new best model\n",
      "Epoch 27/75  Train Loss: 2.903  Val Loss: 2.638\n",
      "Saved new best model\n",
      "Epoch 28/75  Train Loss: 2.835  Val Loss: 2.550\n",
      "Saved new best model\n",
      "Epoch 29/75  Train Loss: 2.726  Val Loss: 2.412\n",
      "Saved new best model\n",
      "Epoch 30/75  Train Loss: 2.636  Val Loss: 2.311\n",
      "Saved new best model\n",
      "Epoch 31/75  Train Loss: 2.530  Val Loss: 2.208\n",
      "Saved new best model\n",
      "Epoch 32/75  Train Loss: 2.394  Val Loss: 2.054\n",
      "Saved new best model\n",
      "Epoch 33/75  Train Loss: 2.327  Val Loss: 1.993\n",
      "Saved new best model\n",
      "Epoch 34/75  Train Loss: 2.232  Val Loss: 1.887\n",
      "Saved new best model\n",
      "Epoch 35/75  Train Loss: 2.139  Val Loss: 1.772\n",
      "Saved new best model\n",
      "Epoch 36/75  Train Loss: 1.999  Val Loss: 1.648\n",
      "Saved new best model\n",
      "Epoch 37/75  Train Loss: 1.890  Val Loss: 1.581\n",
      "Saved new best model\n",
      "Epoch 38/75  Train Loss: 1.861  Val Loss: 1.533\n",
      "Saved new best model\n",
      "Epoch 39/75  Train Loss: 1.757  Val Loss: 1.462\n",
      "Saved new best model\n",
      "Epoch 40/75  Train Loss: 1.732  Val Loss: 1.325\n",
      "Saved new best model\n",
      "Epoch 41/75  Train Loss: 1.613  Val Loss: 1.234\n",
      "Saved new best model\n",
      "Epoch 42/75  Train Loss: 1.504  Val Loss: 1.122\n",
      "Saved new best model\n",
      "Epoch 43/75  Train Loss: 1.433  Val Loss: 1.067\n",
      "Saved new best model\n",
      "Epoch 44/75  Train Loss: 1.297  Val Loss: 0.969\n",
      "Saved new best model\n",
      "Epoch 45/75  Train Loss: 1.191  Val Loss: 0.850\n",
      "Saved new best model\n",
      "Epoch 46/75  Train Loss: 1.109  Val Loss: 0.803\n",
      "Saved new best model\n",
      "Epoch 47/75  Train Loss: 1.044  Val Loss: 0.727\n",
      "Saved new best model\n",
      "Epoch 48/75  Train Loss: 0.989  Val Loss: 0.713\n",
      "Saved new best model\n",
      "Epoch 49/75  Train Loss: 0.980  Val Loss: 0.724\n",
      "Epoch 50/75  Train Loss: 1.017  Val Loss: 0.716\n",
      "Epoch 51/75  Train Loss: 0.933  Val Loss: 0.635\n",
      "Saved new best model\n",
      "Epoch 52/75  Train Loss: 0.823  Val Loss: 0.540\n",
      "Saved new best model\n",
      "Epoch 53/75  Train Loss: 0.717  Val Loss: 0.464\n",
      "Saved new best model\n",
      "Epoch 54/75  Train Loss: 0.642  Val Loss: 0.411\n",
      "Saved new best model\n",
      "Epoch 55/75  Train Loss: 0.577  Val Loss: 0.395\n",
      "Saved new best model\n",
      "Epoch 56/75  Train Loss: 0.537  Val Loss: 0.350\n",
      "Saved new best model\n",
      "Epoch 57/75  Train Loss: 0.575  Val Loss: 0.365\n",
      "Epoch 58/75  Train Loss: 0.515  Val Loss: 0.308\n",
      "Saved new best model\n",
      "Epoch 59/75  Train Loss: 0.460  Val Loss: 0.289\n",
      "Saved new best model\n",
      "Epoch 60/75  Train Loss: 0.433  Val Loss: 0.277\n",
      "Saved new best model\n",
      "Epoch 61/75  Train Loss: 0.425  Val Loss: 0.310\n",
      "Epoch 62/75  Train Loss: 0.408  Val Loss: 0.339\n",
      "Epoch 63/75  Train Loss: 0.573  Val Loss: 0.410\n",
      "Epoch 64/75  Train Loss: 0.517  Val Loss: 0.320\n",
      "Epoch 65/75  Train Loss: 0.435  Val Loss: 0.277\n",
      "Saved new best model\n",
      "Epoch 66/75  Train Loss: 0.403  Val Loss: 0.244\n",
      "Saved new best model\n",
      "Epoch 67/75  Train Loss: 0.376  Val Loss: 0.215\n",
      "Saved new best model\n",
      "Epoch 68/75  Train Loss: 0.323  Val Loss: 0.185\n",
      "Saved new best model\n",
      "Epoch 69/75  Train Loss: 0.294  Val Loss: 0.200\n",
      "Epoch 70/75  Train Loss: 0.446  Val Loss: 0.265\n",
      "Epoch 71/75  Train Loss: 0.376  Val Loss: 0.252\n",
      "Epoch 72/75  Train Loss: 0.327  Val Loss: 0.189\n",
      "Epoch 73/75  Train Loss: 0.305  Val Loss: 0.187\n",
      "Epoch 74/75  Train Loss: 0.281  Val Loss: 0.164\n",
      "Saved new best model\n",
      "Epoch 75/75  Train Loss: 0.247  Val Loss: 0.142\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_435/1183578340.py:302: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base WER,CER: 15.096668037844507 6.0834029906194855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8e57852123406182d9530b78e466ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86af76eee724695b417ade0bf045ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691010f7b4514d2eaacae6b4f74c63d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b635f1ed7d4d5f8a497c667f226d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c577afed5c4b5ea7da0855c40380c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50f189bf4c744348ba582276db42491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b0f2ca7a024183a2710235eadff428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421193bc0d5945dda7db45c4510d3b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb62ee7348c244e59cbc656921fbe271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce73b15b49a3457caaccdaaa3638862d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47b6e7dbf584b8daaf0ecb448e406b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc69998c60f40c0933717b6fe5ebeb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting few-shot LLM refinement...\n",
      "Refining utterances 1-16/215\n",
      "Refining utterances 17-32/215\n",
      "Refining utterances 33-48/215\n",
      "Refining utterances 49-64/215\n",
      "Refining utterances 65-80/215\n",
      "Refining utterances 81-96/215\n",
      "Refining utterances 97-112/215\n",
      "Refining utterances 113-128/215\n",
      "Refining utterances 129-144/215\n",
      "Refining utterances 145-160/215\n",
      "Refining utterances 161-176/215\n",
      "Refining utterances 177-192/215\n",
      "Refining utterances 193-208/215\n",
      "Refining utterances 209-215/215\n",
      "Few-shot done.\n",
      "Clean Few-shot WER,CER: 1390.826820238585 1328.0765301383858\n",
      "\n",
      "Sample 1:\n",
      "ASR      : दोस्तों bashें nested और multvel if statementे spoken tutorial में आपक स्वागत है\n",
      "Few-shot : You are an ASR post-processor. Correct recognition errors in a code-switched transcript. Output only the corrected transcript without any tags or symbols. Example Raw: I am going घर आज Example Corrected: I am going घर आज Raw: दोस्तों bashें nested और multvel if statementे spoken tutorial में आपक स्वागत है। Corrected: FRIENDS bash nested AND multvel if statement spoken tutorial में आपक SWAAGAT है Raw: मैं छोड़ रहूँगा इस ओड़ी से। Corrected: I am leaving this ORDEE . Raw: अगर आप एक डिज्जिटल नम्बर दे दो तो मैं वह दूबाऊंगा। Corrected: IF आप a DIGITAL number give दो TO मैं वह DOUBLE . Raw: ऐसा नहीं है हे दोनो और हो गया चालू है। Corrected: ISA NH HAI HE दोनो AND और HO GAYA CHALU . Raw: हम ने इस टाइम से यह फिल्म देख रहे हैं। Corrected: WE have this TIME SE यह FILM dekh RAH HAI . Raw: मैं हवाई जाऊँगा कल। Corrected: I am going HAVAIII KAL .\n",
      "Reference: दोस्तों bash में nested और multilevel if statement के spoken tutorial में आपका स्वागत है\n",
      "\n",
      "Sample 2:\n",
      "ASR      : इस tutorial में हम निम्न के बारे में सीखेंगे\n",
      "Few-shot : You are an ASR post-processor. Correct recognition errors in a code-switched transcript. Output only the corrected transcript without any tags or symbols. Example Raw: I am going घर आज Example Corrected: I am going घर आज Raw: इस tutorial में हम निम्न के बारे में सीखेंगे Corrected: In tutorial this tutorial we learn about these in manner Raw: I'm going to the store में जा रहा हूँ Corrected: I'm going to the store to the store I'm going Raw: I'm going to the store में जाऊँगा Corrected: I'm going to the store to the store I'm going to Raw: I'm going to the store में जा रहा हूँ Corrected: I'm going to the store to the store I'm going Raw: I'm going to the store में जा रहा हूँ Corrected: I'm going to the store to the store I'm going to the store Raw: I'm going to the store में जा रहा हूँ Corrected: I'm going to the store to the store I'm going Raw: I'm going to the store में जा रहा हूँ Corrected: I'm going to the store to the store I'm going\n",
      "Reference: इस tutorial में हम निम्न के बारे में सीखेंगे\n",
      "\n",
      "Sample 3:\n",
      "ASR      : nested ifelse और\n",
      "Few-shot : nested ifelse और\n",
      "Reference: nested ifelse और\n",
      "\n",
      "Sample 4:\n",
      "ASR      : multilevel ifelse statement\n",
      "Few-shot : multilevel ifelse statement\n",
      "Reference: multilevel ifelse statement\n",
      "\n",
      "Sample 5:\n",
      "ASR      : हम �ह कुछ उदाहर� �पयोग करके करेंगे\n",
      "Few-shot : You are an ASR post-processor. Correct recognition errors in a code-switched transcript. Output only the corrected transcript without any tags or symbols. Example Raw: I am going घर आज Example Corrected: I am going घर आज Raw: हम �ह कुछ उदाहर� �पयोग करके करेंगे यह बात Corrected: hum ha kuch udaahar apyog kar-ke kar-enge yeh baat\n",
      "Reference: हम यह कुछ उदाहरण उपयोग करके करेंगे\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 1: Install dependencies\n",
    "import os\n",
    "# Ensure sentencepiece is available for tokenizer\n",
    "#os.system(\"pip install --quiet sentencepiece\")\n",
    "\n",
    "# %%\n",
    "# Cell 2: Imports & Setup\n",
    "import torchaudio\n",
    "import soundfile as sf  # fallback for wav loading\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Hugging Face token for gated LLM models\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# %%\n",
    "# Cell 3: Utility functions to read Kaldi files\n",
    "def read_wav_scp(path):\n",
    "    wav_dict = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rec_id, wav_path = line.strip().split(maxsplit=1)\n",
    "            wav_dict[rec_id] = wav_path\n",
    "    return wav_dict\n",
    "\n",
    "\n",
    "def read_segments(path):\n",
    "    segments = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            utt_id, rec_id, start, end = parts[0], parts[1], parts[2], parts[3]\n",
    "            segments[utt_id] = (rec_id, float(start), float(end))\n",
    "    return segments\n",
    "\n",
    "\n",
    "def read_text(path):\n",
    "    texts = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            utt_id, transcript = line.strip().split(maxsplit=1)\n",
    "            texts[utt_id] = transcript\n",
    "    return texts\n",
    "\n",
    "# %%\n",
    "# Cell 4: KaldiDataset and collate_fn\n",
    "class KaldiDataset(Dataset):\n",
    "    def __init__(self, data_dir, processor, sample_rate=16000):\n",
    "        self.data_dir = data_dir\n",
    "        self.processor = processor\n",
    "        self.sample_rate = sample_rate\n",
    "        md = f\"{data_dir}/transcripts\"\n",
    "        self.wav_dict = read_wav_scp(f\"{md}/wav.scp\")\n",
    "        self.segments = read_segments(f\"{md}/segments\")\n",
    "        self.texts = read_text(f\"{md}/text\")\n",
    "        self.utt_ids = list(self.texts.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utt_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt_id = self.utt_ids[idx]\n",
    "        rec_id, start, end = self.segments[utt_id]\n",
    "        rel_path = self.wav_dict[rec_id]\n",
    "        # Try locating wav under data_dir, else metadata_dir\n",
    "        wav_path1 = os.path.join(self.data_dir, rel_path)\n",
    "        wav_path2 = os.path.join(os.path.dirname(wav_path1), rel_path)\n",
    "        if os.path.isfile(wav_path1):\n",
    "            wav_path = wav_path1\n",
    "        elif os.path.isfile(wav_path2):\n",
    "            wav_path = wav_path2\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Audio file not found: {wav_path1} or {wav_path2}\")\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(wav_path)\n",
    "        except Exception:\n",
    "            # Fallback to soundfile\n",
    "            waveform_np, sr = sf.read(wav_path)\n",
    "            # Convert numpy to torch tensor with shape [channels, time]\n",
    "            if waveform_np.ndim > 1:\n",
    "                waveform = torch.from_numpy(wavfile_np.T)\n",
    "            else:\n",
    "                waveform = torch.from_numpy(wavfile_np).unsqueeze(0)\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "        segment = waveform[0, int(start*self.sample_rate):int(end*self.sample_rate)]\n",
    "        feat = self.processor.feature_extractor(\n",
    "            segment.numpy(), sampling_rate=self.sample_rate, return_tensors=\"pt\"\n",
    "        ).input_features[0]\n",
    "        labels = self.processor.tokenizer(\n",
    "            self.texts[utt_id], return_tensors=\"pt\", add_special_tokens=False\n",
    "        ).input_ids[0]\n",
    "        return {\"utt_id\": utt_id, \"input_features\": feat, \"labels\": labels}\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    feats = [b[\"input_features\"] for b in batch]\n",
    "    labs = [b[\"labels\"] for b in batch]\n",
    "    ids = [b[\"utt_id\"] for b in batch]\n",
    "    feats_p = nn.utils.rnn.pad_sequence(feats, batch_first=True)\n",
    "    labs_p = nn.utils.rnn.pad_sequence(labs, batch_first=True, padding_value=-100)\n",
    "    return {\"utt_ids\": ids, \"input_features\": feats_p, \"labels\": labs_p}\n",
    "\n",
    "# %%\n",
    "# Cell 5: Define WhisperMoEModel\n",
    "class WhisperMoEModel(nn.Module):\n",
    "    def __init__(self, whisper_encoder, d_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.whisper_encoder = whisper_encoder\n",
    "        self.expert_m = nn.Linear(d_model, d_model)\n",
    "        self.expert_e = nn.Linear(d_model, d_model)\n",
    "        self.gate = nn.Linear(2*d_model, 2)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        out = self.whisper_encoder(input_features=input_features)\n",
    "        shared = out.last_hidden_state\n",
    "        m = self.expert_m(shared)\n",
    "        e = self.expert_e(shared)\n",
    "        cat = torch.cat([m, e], dim=-1)\n",
    "        g = self.gate(cat)\n",
    "        w = torch.softmax(g, dim=-1)\n",
    "        wm, we = w[..., 0:1], w[..., 1:2]\n",
    "        mix = wm*m + we*e\n",
    "        logits = self.classifier(mix)\n",
    "        return logits, w\n",
    "\n",
    "# %%\n",
    "# Cell 6: CTC decode and metrics\n",
    "def ctc_greedy_decode(logits, blank_id):\n",
    "    ids = logits.argmax(dim=-1).cpu().tolist()\n",
    "    prev, out = None, []\n",
    "    for i in ids:\n",
    "        if i != prev and i != blank_id:\n",
    "            out.append(i)\n",
    "        prev = i\n",
    "    return out\n",
    "\n",
    "\n",
    "def edit_distance(r, h):\n",
    "    m, n = len(r), len(h)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m+1): dp[i][0] = i\n",
    "    for j in range(n+1): dp[0][j] = j\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            dp[i][j] = dp[i-1][j-1] if r[i-1]==h[j-1] else 1+min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "def compute_wer(preds, refs):\n",
    "    te, tw = 0, 0\n",
    "    for r, h in zip(refs, preds):\n",
    "        rw, hw = r.split(), h.split()\n",
    "        te += edit_distance(rw, hw); tw += len(rw)\n",
    "    return te/tw*100 if tw>0 else 0\n",
    "\n",
    "\n",
    "def compute_cer(preds, refs):\n",
    "    te, tc = 0, 0\n",
    "    for r, h in zip(refs, preds):\n",
    "        rc, hc = list(r.replace(\" \",\"\")), list(h.replace(\" \",\"\"))\n",
    "        te += edit_distance(rc, hc); tc += len(rc)\n",
    "    return te/tc*100 if tc>0 else 0\n",
    "\n",
    "# %%\n",
    "# Cell 7: LLM refine function with batching, filtering & progress\n",
    "def refine_with_llm(raw_texts, tokenizer_llm, model_llm, gen_conf, few_shot=None, batch_size=16, min_length=6):\n",
    "    \"\"\"\n",
    "    Only refine utterances longer than min_length tokens.\n",
    "    Short utterances are left unchanged.\n",
    "    \"\"\"\n",
    "    device = next(model_llm.parameters()).device\n",
    "    refined = list(raw_texts)\n",
    "    to_refine = [i for i, txt in enumerate(raw_texts) if len(txt.split())>=min_length]\n",
    "    N = len(to_refine)\n",
    "    for start in range(0, N, batch_size):\n",
    "        batch_idxs = to_refine[start:start+batch_size]\n",
    "        prompts = []\n",
    "        for idx in batch_idxs:\n",
    "            raw = raw_texts[idx]\n",
    "            lines = [\n",
    "                \"You are an ASR post-processor. Correct recognition errors in a code-switched transcript.\",\n",
    "                \"Output only the corrected transcript without any tags or symbols.\"\n",
    "            ]\n",
    "            if few_shot:\n",
    "                for gold, example in few_shot:\n",
    "                    lines.append(f\"Example Raw: {example}\")\n",
    "                    lines.append(f\"Example Corrected: {gold}\")\n",
    "            lines.append(f\"Raw: {raw}\")\n",
    "            prompts.append(\"\\n\".join(lines))\n",
    "        print(f\"Refining utterances {start+1}-{start+len(batch_idxs)}/{N}\")\n",
    "        if tokenizer_llm.pad_token_id is None:\n",
    "            tokenizer_llm.pad_token = tokenizer_llm.eos_token\n",
    "        inputs = tokenizer_llm(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        outputs = model_llm.generate(\n",
    "            **inputs,\n",
    "            generation_config=gen_conf,\n",
    "            pad_token_id=tokenizer_llm.eos_token_id\n",
    "        )\n",
    "        for i_out, output in enumerate(outputs):\n",
    "            text = tokenizer_llm.decode(output, skip_special_tokens=True).strip()\n",
    "            refined[batch_idxs[i_out]] = text\n",
    "    return refined\n",
    "\n",
    "# %%\n",
    "# Cell 8: Train/validate/evaluate loops\n",
    "def train_epoch(model, loader, optimizer, criterion, device, blank_id):\n",
    "    model.train(); total=0\n",
    "    for b in loader:\n",
    "        feats=b[\"input_features\"].to(device)\n",
    "        labs=b[\"labels\"].to(device)\n",
    "        logits,_=model(feats)\n",
    "        logp=F.log_softmax(logits,dim=-1).transpose(0,1)\n",
    "        B=logp.size(1)\n",
    "        in_l=torch.full((B,), logp.size(0), dtype=torch.long).to(device)\n",
    "        tgt_l=(labs!=-100).sum(dim=1).to(device)\n",
    "        labs_ctc=labs.masked_fill(labs==-100, blank_id)\n",
    "        loss=criterion(logp, labs_ctc, in_l, tgt_l)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        total+=loss.item()\n",
    "    return total/len(loader)\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device, blank_id):\n",
    "    model.eval(); total=0\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            feats=b[\"input_features\"].to(device)\n",
    "            labs=b[\"labels\"].to(device)\n",
    "            logits,_=model(feats)\n",
    "            logp=F.log_softmax(logits,dim=-1).transpose(0,1)\n",
    "            B=logp.size(1)\n",
    "            in_l=torch.full((B,), logp.size(0), dtype=torch.long).to(device)\n",
    "            tgt_l=(labs!=-100).sum(dim=1).to(device)\n",
    "            labs_ctc=labs.masked_fill(labs==-100, blank_id)\n",
    "            total+=criterion(logp, labs_ctc, in_l, tgt_l).item()\n",
    "    return total/len(loader)\n",
    "\n",
    "def evaluate(model, loader, processor, device, blank_id):\n",
    "    model.eval(); preds, refs = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            feats=b[\"input_features\"].to(device)\n",
    "            labs=b[\"labels\"]\n",
    "            logits,_=model(feats)\n",
    "            for i in range(logits.size(0)):\n",
    "                ids=ctc_greedy_decode(logits[i], blank_id)\n",
    "                preds.append(processor.tokenizer.decode(ids, skip_special_tokens=True))\n",
    "            for l in labs:\n",
    "                l=l.clone().masked_fill(l==-100, blank_id)\n",
    "                refs.append(processor.tokenizer.decode(l.cpu().tolist(), skip_special_tokens=True))\n",
    "    return preds, refs\n",
    "\n",
    "# %%\n",
    "# Cell 9: Main + LLM post-processing\n",
    "def main():\n",
    "    train_dir, test_dir = \"train_split\", \"test_split\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "    blank_id = processor.tokenizer.pad_token_id\n",
    "    whisper = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "    d_model = whisper.config.d_model\n",
    "    num_classes = processor.tokenizer.vocab_size\n",
    "\n",
    "    tr_ds = KaldiDataset(train_dir, processor)\n",
    "    te_ds = KaldiDataset(test_dir, processor)\n",
    "    tr_ld = DataLoader(tr_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "    te_ld = DataLoader(te_ds, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = WhisperMoEModel(whisper.encoder, d_model, num_classes).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CTCLoss(blank=blank_id, zero_infinity=True)\n",
    "\n",
    "    # Train Whisper+MoE model\n",
    "    best_val = float('inf')\n",
    "    for ep in range(1, 76):\n",
    "        # Freeze encoder first 3 epochs\n",
    "        for p in model.whisper_encoder.parameters():\n",
    "            p.requires_grad = (ep > 3)\n",
    "        tr_loss = train_epoch(model, tr_ld, optimizer, criterion, device, blank_id)\n",
    "        val_loss = validate_epoch(model, te_ld, criterion, device, blank_id)\n",
    "        print(f\"Epoch {ep}/75  Train Loss: {tr_loss:.3f}  Val Loss: {val_loss:.3f}\")\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), \"best.pth\")\n",
    "            print(\"Saved new best model\")\n",
    "\n",
    "    # Load the best trained model\n",
    "    model.load_state_dict(torch.load(\"best.pth\"))\n",
    "\n",
    "    # Evaluate base\n",
    "    preds, refs = evaluate(model, te_ld, processor, device, blank_id)\n",
    "    print(\"Base WER,CER:\", compute_wer(preds, refs), compute_cer(preds, refs))\n",
    "\n",
    "    # LLM few-shot refinement only\n",
    "    llm_nm = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    tok_llm = AutoTokenizer.from_pretrained(llm_nm, trust_remote_code=True, token=HF_TOKEN)\n",
    "    mlm = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_nm, device_map=\"auto\", torch_dtype=torch.float16,\n",
    "        trust_remote_code=True, token=HF_TOKEN\n",
    "    )\n",
    "    gen_cfg = GenerationConfig(max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "    examples = [(\"I EN | am EN | going EN | घर HI | आज HI\", \"I am going घर आज\")]\n",
    "\n",
    "    print(\"Starting few-shot LLM refinement...\")\n",
    "    fs = refine_with_llm(preds, tok_llm, mlm, gen_cfg, few_shot=examples)\n",
    "    print(\"Few-shot done.\")\n",
    "\n",
    "    # Strip tags & compute final metrics\n",
    "    def strip_llm_tags(text):\n",
    "        cleaned = re.sub(r\"(\\bEN\\b|\\bHI\\b|\\||,)\", \"\", text)\n",
    "        return \" \".join(cleaned.split())\n",
    "    clean_fs = [strip_llm_tags(t) for t in fs]\n",
    "    print(\"Clean Few-shot WER,CER:\", compute_wer(clean_fs, refs), compute_cer(clean_fs, refs))\n",
    "\n",
    "    # Show sample outputs\n",
    "    for i in range(min(5, len(preds))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(\"ASR      :\", preds[i])\n",
    "        print(\"Few-shot :\", clean_fs[i])\n",
    "        print(\"Reference:\", refs[i])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50bcdc1",
   "metadata": {},
   "source": [
    "# Full hybrid pipeline v2 (tighter, 50 epochs) → start LLM few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "444c8a58-7200-4dbd-9d3d-4c10c7f55671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50  Train Loss: 292.826  Val Loss: 131.995\n",
      "Saved new best model\n",
      "Epoch 2/50  Train Loss: 78.031  Val Loss: 58.970\n",
      "Saved new best model\n",
      "Epoch 3/50  Train Loss: 37.485  Val Loss: 33.101\n",
      "Saved new best model\n",
      "Epoch 4/50  Train Loss: 10.436  Val Loss: 5.463\n",
      "Saved new best model\n",
      "Epoch 5/50  Train Loss: 4.972  Val Loss: 4.843\n",
      "Saved new best model\n",
      "Epoch 6/50  Train Loss: 4.621  Val Loss: 4.649\n",
      "Saved new best model\n",
      "Epoch 7/50  Train Loss: 4.454  Val Loss: 4.627\n",
      "Saved new best model\n",
      "Epoch 8/50  Train Loss: 4.346  Val Loss: 4.389\n",
      "Saved new best model\n",
      "Epoch 9/50  Train Loss: 4.232  Val Loss: 4.230\n",
      "Saved new best model\n",
      "Epoch 10/50  Train Loss: 4.104  Val Loss: 4.093\n",
      "Saved new best model\n",
      "Epoch 11/50  Train Loss: 3.962  Val Loss: 3.868\n",
      "Saved new best model\n",
      "Epoch 12/50  Train Loss: 3.645  Val Loss: 3.320\n",
      "Saved new best model\n",
      "Epoch 13/50  Train Loss: 3.039  Val Loss: 2.674\n",
      "Saved new best model\n",
      "Epoch 14/50  Train Loss: 2.473  Val Loss: 2.282\n",
      "Saved new best model\n",
      "Epoch 15/50  Train Loss: 2.009  Val Loss: 1.756\n",
      "Saved new best model\n",
      "Epoch 16/50  Train Loss: 1.638  Val Loss: 1.486\n",
      "Saved new best model\n",
      "Epoch 17/50  Train Loss: 1.353  Val Loss: 1.232\n",
      "Saved new best model\n",
      "Epoch 18/50  Train Loss: 1.132  Val Loss: 1.061\n",
      "Saved new best model\n",
      "Epoch 19/50  Train Loss: 0.992  Val Loss: 0.866\n",
      "Saved new best model\n",
      "Epoch 20/50  Train Loss: 0.822  Val Loss: 0.747\n",
      "Saved new best model\n",
      "Epoch 21/50  Train Loss: 0.687  Val Loss: 0.612\n",
      "Saved new best model\n",
      "Epoch 22/50  Train Loss: 0.569  Val Loss: 0.541\n",
      "Saved new best model\n",
      "Epoch 23/50  Train Loss: 0.488  Val Loss: 0.444\n",
      "Saved new best model\n",
      "Epoch 24/50  Train Loss: 0.411  Val Loss: 0.377\n",
      "Saved new best model\n",
      "Epoch 25/50  Train Loss: 0.354  Val Loss: 0.308\n",
      "Saved new best model\n",
      "Epoch 26/50  Train Loss: 0.293  Val Loss: 0.269\n",
      "Saved new best model\n",
      "Epoch 27/50  Train Loss: 0.245  Val Loss: 0.212\n",
      "Saved new best model\n",
      "Epoch 28/50  Train Loss: 0.201  Val Loss: 0.176\n",
      "Saved new best model\n",
      "Epoch 29/50  Train Loss: 0.164  Val Loss: 0.141\n",
      "Saved new best model\n",
      "Epoch 30/50  Train Loss: 0.140  Val Loss: 0.121\n",
      "Saved new best model\n",
      "Epoch 31/50  Train Loss: 0.112  Val Loss: 0.095\n",
      "Saved new best model\n",
      "Epoch 32/50  Train Loss: 0.091  Val Loss: 0.079\n",
      "Saved new best model\n",
      "Epoch 33/50  Train Loss: 0.075  Val Loss: 0.069\n",
      "Saved new best model\n",
      "Epoch 34/50  Train Loss: 0.063  Val Loss: 0.058\n",
      "Saved new best model\n",
      "Epoch 35/50  Train Loss: 0.052  Val Loss: 0.050\n",
      "Saved new best model\n",
      "Epoch 36/50  Train Loss: 0.047  Val Loss: 0.046\n",
      "Saved new best model\n",
      "Epoch 37/50  Train Loss: 0.046  Val Loss: 0.045\n",
      "Saved new best model\n",
      "Epoch 38/50  Train Loss: 0.087  Val Loss: 0.201\n",
      "Epoch 39/50  Train Loss: 0.444  Val Loss: 0.399\n",
      "Epoch 40/50  Train Loss: 0.546  Val Loss: 0.261\n",
      "Epoch 41/50  Train Loss: 0.283  Val Loss: 0.118\n",
      "Epoch 42/50  Train Loss: 0.149  Val Loss: 0.098\n",
      "Epoch 43/50  Train Loss: 0.076  Val Loss: 0.061\n",
      "Epoch 44/50  Train Loss: 0.059  Val Loss: 0.049\n",
      "Epoch 45/50  Train Loss: 0.119  Val Loss: 0.083\n",
      "Epoch 46/50  Train Loss: 0.069  Val Loss: 0.055\n",
      "Epoch 47/50  Train Loss: 0.041  Val Loss: 0.029\n",
      "Saved new best model\n",
      "Epoch 48/50  Train Loss: 0.027  Val Loss: 0.022\n",
      "Saved new best model\n",
      "Epoch 49/50  Train Loss: 0.021  Val Loss: 0.021\n",
      "Saved new best model\n",
      "Epoch 50/50  Train Loss: 0.018  Val Loss: 0.015\n",
      "Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_435/2687253054.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base WER,CER: 0.20567667626491154 0.07430110522894028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0e7646a90b4e01bd4513162444bc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting few-shot LLM refinement...\n",
      "Refining utterances 1-16\n",
      "Refining utterances 17-32\n",
      "Refining utterances 33-48\n",
      "Refining utterances 49-64\n",
      "Refining utterances 65-80\n",
      "Refining utterances 81-96\n",
      "Refining utterances 97-112\n",
      "Refining utterances 113-128\n",
      "Refining utterances 129-144\n",
      "Refining utterances 145-160\n",
      "Refining utterances 161-176\n",
      "Refining utterances 177-192\n",
      "Refining utterances 193-208\n",
      "Refining utterances 209-214\n",
      "Few-shot done.\n",
      "Clean Few-shot WER,CER: 1295.7630604689427 1282.9571839881119\n",
      "\n",
      "Sample 1:\n",
      "ASR      : दोस्तों bash में nested और multilevel if statement के spoken tutorial में आपका स्वागत है\n",
      "Few-shot : You are an ASR post-processor. Correct recognition errors and output just the corrected sentence. Example Raw: I am going घर आज Example Corrected: I am going घर आज Raw: दोस्तों bash में nested और multilevel if statement के spoken tutorial में आपका स्वागत है Corrected: FRIENDS in BASH nested AND multilevel IF STATEMENT spoken TUTORIAL is your welcome Raw: सब कुछ कैसे हो रहा है Corrected: EVERYTHING how is happening Raw: आज तो इस बात के बारे में कहोगा Corrected: TODAY then about this matter tell Raw: आज आप क्यों छोड़े हैं यह क्यों है Corrected: TODAY you why left this why is Raw: एक बार फिर आप कहेंगे यह क्यों है Corrected: ONCE AGAIN you will say this why is Raw: यह मेरा प्रश्न है क्यों यह हो रहा है Corrected: THIS is my question why this is happening\n",
      "Reference: दोस्तों bash में nested और multilevel if statement के spoken tutorial में आपका स्वागत है\n",
      "\n",
      "Sample 2:\n",
      "ASR      : इस tutorial में हम निम्न के बारे में सीखेंगे\n",
      "Few-shot : You are an ASR post-processor. Correct recognition errors and output just the corrected sentence. Example Raw: I am going घर आज Example Corrected: I am going घर आज Raw: इस tutorial में हम निम्न के बारे में सीखेंगे Corrected: In this tutorial we will learn these about Raw: और इसके लिए इस टीम के साथ काम करेंगे Corrected: And for this we will work with this team Raw: इसको देख ले कि तू स्प्रिट है या नहीं Corrected: Look at this to see if you are a spirit or not Raw: हम कुछ महत्वपूर्ण बात बताते हैं Corrected: We are discussing some important matters Raw: क्यों नहीं तू मुझे समझ सकता Corrected: Why can't you understand ME Raw: आप क्या कर रहे हैं Corrected: What are you doing\n",
      "Reference: इस tutorial में हम निम्न के बारे में सीखेंगे\n",
      "\n",
      "Sample 3:\n",
      "ASR      : nested ifelse और\n",
      "Few-shot : nested ifelse और\n",
      "Reference: nested ifelse और\n",
      "\n",
      "Sample 4:\n",
      "ASR      : multilevel ifelse statement\n",
      "Few-shot : multilevel ifelse statement\n",
      "Reference: multilevel ifelse statement\n",
      "\n",
      "Sample 5:\n",
      "ASR      : हम यह कुछ उदाहरण उपयोग करके करेंगे\n",
      "Few-shot : You are an ASR post-processor. Correct recognition errors and output just the corrected sentence. Example Raw: I am going घर आज Example Corrected: I am going घर आज Raw: हम यह कुछ उदाहरण उपयोग करके करेंगे Corrected: WE this some example use do करेंगे Raw: आप अपना फोन से मुझपे कॉल कर ठीक करें Corrected: YOU your phone from ME call do correct Raw: इस साठ हर घदी के साथ काम कर रहा हूँ Corrected: I this साठ every minute with work do am\n",
      "Reference: हम यह कुछ उदाहरण उपयोग करके करेंगे\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 1: Install dependencies\n",
    "import os\n",
    "# Ensure sentencepiece is available for tokenizer\n",
    "os.system(\"pip install --quiet sentencepiece\")\n",
    "\n",
    "# %%\n",
    "# Cell 2: Imports & Setup\n",
    "import torchaudio\n",
    "import soundfile as sf  # fallback for wav loading\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Hugging Face token for gated LLM models\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# %%\n",
    "# Cell 3: Utility functions to read Kaldi files\n",
    "def read_wav_scp(path):\n",
    "    wav_dict = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rec_id, wav_path = line.strip().split(maxsplit=1)\n",
    "            wav_dict[rec_id] = wav_path\n",
    "    return wav_dict\n",
    "\n",
    "\n",
    "def read_segments(path):\n",
    "    segments = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            utt_id, rec_id, start, end = parts[0], parts[1], parts[2], parts[3]\n",
    "            segments[utt_id] = (rec_id, float(start), float(end))\n",
    "    return segments\n",
    "\n",
    "\n",
    "def read_text(path):\n",
    "    texts = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            utt_id, transcript = line.strip().split(maxsplit=1)\n",
    "            texts[utt_id] = transcript\n",
    "    return texts\n",
    "\n",
    "# %%\n",
    "# Cell 4: KaldiDataset and collate_fn\n",
    "class KaldiDataset(Dataset):\n",
    "    def __init__(self, data_dir, processor, sample_rate=16000):\n",
    "        self.data_dir = data_dir\n",
    "        self.processor = processor\n",
    "        self.sample_rate = sample_rate\n",
    "        md = f\"{data_dir}/transcripts\"\n",
    "        self.wav_dict = read_wav_scp(f\"{md}/wav.scp\")\n",
    "        self.segments = read_segments(f\"{md}/segments\")\n",
    "        self.texts = read_text(f\"{md}/text\")\n",
    "        self.utt_ids = list(self.texts.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utt_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt_id = self.utt_ids[idx]\n",
    "        rec_id, start, end = self.segments[utt_id]\n",
    "        rel = self.wav_dict[rec_id]\n",
    "\n",
    "        # Try primary and fallback paths\n",
    "        p1 = os.path.join(self.data_dir, rel)\n",
    "        p2 = os.path.join(self.data_dir, \"transcripts\", rel)\n",
    "        if os.path.isfile(p1):\n",
    "            wav_path = p1\n",
    "        elif os.path.isfile(p2):\n",
    "            wav_path = p2\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Missing audio: {p1} or {p2}\")\n",
    "\n",
    "        # Load with torchaudio or fallback to soundfile\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(wav_path)\n",
    "        except:\n",
    "            arr, sr = sf.read(wav_path)\n",
    "            arr = np.asarray(arr, dtype=np.float32)\n",
    "            if arr.ndim == 1:\n",
    "                waveform = torch.from_numpy(arr).unsqueeze(0)\n",
    "            else:\n",
    "                waveform = torch.from_numpy(arr.T)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "        # Slice segment, extract features, tokenize\n",
    "        segment = waveform[0, int(start*self.sample_rate):int(end*self.sample_rate)]\n",
    "        feat = self.processor.feature_extractor(\n",
    "            segment.numpy(),\n",
    "            sampling_rate=self.sample_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features[0]\n",
    "        labels = self.processor.tokenizer(\n",
    "            self.texts[utt_id],\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False\n",
    "        ).input_ids[0]\n",
    "\n",
    "        return {\n",
    "            \"utt_id\": utt_id,\n",
    "            \"input_features\": feat,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    feats = [b[\"input_features\"] for b in batch]\n",
    "    labs = [b[\"labels\"] for b in batch]\n",
    "    ids = [b[\"utt_id\"] for b in batch]\n",
    "    feats_p = nn.utils.rnn.pad_sequence(feats, batch_first=True)\n",
    "    labs_p = nn.utils.rnn.pad_sequence(labs, batch_first=True, padding_value=-100)\n",
    "    return {\"utt_ids\": ids, \"input_features\": feats_p, \"labels\": labs_p}\n",
    "\n",
    "# %%\n",
    "# Cell 5: Define WhisperMoEModel\n",
    "class WhisperMoEModel(nn.Module):\n",
    "    def __init__(self, whisper_encoder, d_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.whisper_encoder = whisper_encoder\n",
    "        self.expert_m = nn.Linear(d_model, d_model)\n",
    "        self.expert_e = nn.Linear(d_model, d_model)\n",
    "        self.gate = nn.Linear(2*d_model, 2)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        out = self.whisper_encoder(input_features=input_features)\n",
    "        shared = out.last_hidden_state\n",
    "        m = self.expert_m(shared)\n",
    "        e = self.expert_e(shared)\n",
    "        cat = torch.cat([m, e], dim=-1)\n",
    "        g = self.gate(cat)\n",
    "        w = torch.softmax(g, dim=-1)\n",
    "        wm, we = w[..., 0:1], w[..., 1:2]\n",
    "        mix = wm*m + we*e\n",
    "        logits = self.classifier(mix)\n",
    "        return logits, w\n",
    "\n",
    "# %%\n",
    "# Cell 6: CTC decode and metrics\n",
    "def ctc_greedy_decode(logits, blank_id):\n",
    "    ids = logits.argmax(dim=-1).cpu().tolist()\n",
    "    prev, out = None, []\n",
    "    for i in ids:\n",
    "        if i != prev and i != blank_id:\n",
    "            out.append(i)\n",
    "        prev = i\n",
    "    return out\n",
    "\n",
    "\n",
    "def edit_distance(r, h):\n",
    "    m, n = len(r), len(h)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m+1): dp[i][0] = i\n",
    "    for j in range(n+1): dp[0][j] = j\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            dp[i][j] = dp[i-1][j-1] if r[i-1]==h[j-1] else 1+min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "def compute_wer(preds, refs):\n",
    "    te, tw = 0, 0\n",
    "    for r, h in zip(refs, preds):\n",
    "        rw, hw = r.split(), h.split()\n",
    "        te += edit_distance(rw, hw); tw += len(rw)\n",
    "    return te/tw*100 if tw>0 else 0\n",
    "\n",
    "\n",
    "def compute_cer(preds, refs):\n",
    "    te, tc = 0, 0\n",
    "    for r, h in zip(refs, preds):\n",
    "        rc, hc = list(r.replace(\" \",\"\")), list(h.replace(\" \",\"\"))\n",
    "        te += edit_distance(rc, hc); tc += len(rc)\n",
    "    return te/tc*100 if tc>0 else 0\n",
    "\n",
    "# %%\n",
    "# Cell 7: LLM refine function with batching, filtering & progress\n",
    "def refine_with_llm(raw_texts, tokenizer_llm, model_llm, gen_conf,\n",
    "                    few_shot=None, batch_size=16, min_length=6):\n",
    "    device = next(model_llm.parameters()).device\n",
    "    refined = list(raw_texts)\n",
    "    to_refine = [i for i, t in enumerate(raw_texts) if len(t.split()) >= min_length]\n",
    "\n",
    "    for start in range(0, len(to_refine), batch_size):\n",
    "        batch_idxs = to_refine[start:start + batch_size]\n",
    "        prompts = []\n",
    "        for idx in batch_idxs:\n",
    "            lines = [\n",
    "                \"You are an ASR post-processor. Correct recognition errors and output just the corrected sentence.\",\n",
    "            ]\n",
    "            if few_shot:\n",
    "                for gold, example in few_shot:\n",
    "                    lines.append(f\"Example Raw: {example}\")\n",
    "                    lines.append(f\"Example Corrected: {gold}\")\n",
    "            lines.append(f\"Raw: {raw_texts[idx]}\")\n",
    "            lines.append(\"Corrected:\")\n",
    "            prompts.append(\"\\n\".join(lines))\n",
    "\n",
    "        print(f\"Refining utterances {start+1}-{start+len(batch_idxs)}\")\n",
    "        if tokenizer_llm.pad_token_id is None:\n",
    "            tokenizer_llm.pad_token = tokenizer_llm.eos_token\n",
    "\n",
    "        inputs = tokenizer_llm(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        outputs = model_llm.generate(\n",
    "            **inputs,\n",
    "            generation_config=gen_conf,\n",
    "            pad_token_id=tokenizer_llm.eos_token_id,\n",
    "        )\n",
    "        for i, out in enumerate(outputs):\n",
    "            text = tokenizer_llm.decode(out, skip_special_tokens=True).strip()\n",
    "            refined[batch_idxs[i]] = text\n",
    "\n",
    "    return refined\n",
    "\n",
    "# %%\n",
    "# Cell 8: Train/validate/evaluate loops\n",
    "def train_epoch(model, loader, optimizer, criterion, device, blank_id):\n",
    "    model.train(); total=0\n",
    "    for b in loader:\n",
    "        feats=b[\"input_features\"].to(device)\n",
    "        labs=b[\"labels\"].to(device)\n",
    "        logits,_=model(feats)\n",
    "        logp=F.log_softmax(logits,dim=-1).transpose(0,1)\n",
    "        B=logp.size(1)\n",
    "        in_l=torch.full((B,), logp.size(0), dtype=torch.long).to(device)\n",
    "        tgt_l=(labs!=-100).sum(dim=1).to(device)\n",
    "        labs_ctc=labs.masked_fill(labs==-100, blank_id)\n",
    "        loss=criterion(logp, labs_ctc, in_l, tgt_l)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        total+=loss.item()\n",
    "    return total/len(loader)\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device, blank_id):\n",
    "    model.eval(); total=0\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            feats=b[\"input_features\"].to(device)\n",
    "            labs=b[\"labels\"].to(device)\n",
    "            logits,_=model(feats)\n",
    "            logp=F.log_softmax(logits,dim=-1).transpose(0,1)\n",
    "            B=logp.size(1)\n",
    "            in_l=torch.full((B,), logp.size(0), dtype=torch.long).to(device)\n",
    "            tgt_l=(labs!=-100).sum(dim=1).to(device)\n",
    "            labs_ctc=labs.masked_fill(labs==-100, blank_id)\n",
    "            total+=criterion(logp, labs_ctc, in_l, tgt_l).item()\n",
    "    return total/len(loader)\n",
    "\n",
    "def evaluate(model, loader, processor, device, blank_id):\n",
    "    model.eval(); preds, refs = [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            feats=b[\"input_features\"].to(device)\n",
    "            labs=b[\"labels\"]\n",
    "            logits,_=model(feats)\n",
    "            for i in range(logits.size(0)):\n",
    "                ids=ctc_greedy_decode(logits[i], blank_id)\n",
    "                preds.append(processor.tokenizer.decode(ids, skip_special_tokens=True))\n",
    "            for l in labs:\n",
    "                l=l.clone().masked_fill(l==-100, blank_id)\n",
    "                refs.append(processor.tokenizer.decode(l.cpu().tolist(), skip_special_tokens=True))\n",
    "    return preds, refs\n",
    "\n",
    "# %%\n",
    "# Cell 9: Main + LLM post-processing\n",
    "def main():\n",
    "    train_dir, test_dir = \"train_split\", \"test_split\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "    blank_id = processor.tokenizer.pad_token_id\n",
    "    whisper = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "    d_model = whisper.config.d_model\n",
    "    num_classes = processor.tokenizer.vocab_size\n",
    "\n",
    "    tr_ds = KaldiDataset(train_dir, processor)\n",
    "    te_ds = KaldiDataset(test_dir, processor)\n",
    "    tr_ld = DataLoader(tr_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "    te_ld = DataLoader(te_ds, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = WhisperMoEModel(whisper.encoder, d_model, num_classes).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CTCLoss(blank=blank_id, zero_infinity=True)\n",
    "\n",
    "    # Train Whisper+MoE model\n",
    "    best_val = float('inf')\n",
    "    for ep in range(1, 51):\n",
    "        # Freeze encoder first 3 epochs\n",
    "        for p in model.whisper_encoder.parameters():\n",
    "            p.requires_grad = (ep > 3)\n",
    "        tr_loss = train_epoch(model, tr_ld, optimizer, criterion, device, blank_id)\n",
    "        val_loss = validate_epoch(model, te_ld, criterion, device, blank_id)\n",
    "        print(f\"Epoch {ep}/50  Train Loss: {tr_loss:.3f}  Val Loss: {val_loss:.3f}\")\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), \"best.pth\")\n",
    "            print(\"Saved new best model\")\n",
    "\n",
    "    # Load the best trained model\n",
    "    model.load_state_dict(torch.load(\"best.pth\"))\n",
    "\n",
    "    # Evaluate base\n",
    "    preds, refs = evaluate(model, te_ld, processor, device, blank_id)\n",
    "    print(\"Base WER,CER:\", compute_wer(preds, refs), compute_cer(preds, refs))\n",
    "\n",
    "    # LLM few-shot refinement only\n",
    "    llm_nm = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    tok_llm = AutoTokenizer.from_pretrained(llm_nm, trust_remote_code=True, token=HF_TOKEN)\n",
    "    mlm = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_nm, device_map=\"auto\", torch_dtype=torch.float16,\n",
    "        trust_remote_code=True, token=HF_TOKEN\n",
    "    )\n",
    "    gen_cfg = GenerationConfig(max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "    examples = [(\"I EN | am EN | going EN | घर HI | आज HI\", \"I am going घर आज\")]\n",
    "\n",
    "    print(\"Starting few-shot LLM refinement...\")\n",
    "    fs = refine_with_llm(preds, tok_llm, mlm, gen_cfg, few_shot=examples)\n",
    "    print(\"Few-shot done.\")\n",
    "\n",
    "    # Strip tags & compute final metrics\n",
    "    def strip_llm_tags(text):\n",
    "        cleaned = re.sub(r\"(\\bEN\\b|\\bHI\\b|\\||,)\", \"\", text)\n",
    "        return \" \".join(cleaned.split())\n",
    "    clean_fs = [strip_llm_tags(t) for t in fs]\n",
    "    print(\"Clean Few-shot WER,CER:\", compute_wer(clean_fs, refs), compute_cer(clean_fs, refs))\n",
    "\n",
    "    # Show sample outputs\n",
    "    for i in range(min(5, len(preds))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(\"ASR      :\", preds[i])\n",
    "        print(\"Few-shot :\", clean_fs[i])\n",
    "        print(\"Reference:\", refs[i])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c2823",
   "metadata": {},
   "source": [
    "# LLM refine helper (variant)\n",
    "\n",
    "- Defines a stricter refine_with_llm batched generator (min length filter, prompt that asks for compact outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0baee7d6-1d14-4687-aec4-80a55a126dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: LLM refine function with batching, filtering & progress\n",
    "def refine_with_llm(raw_texts, tokenizer_llm, model_llm, gen_conf, few_shot=None, batch_size=16, min_length=6):\n",
    "    \"\"\"\n",
    "    Only refine utterances longer than min_length tokens.\n",
    "    Short utterances are left unchanged.\n",
    "    Uses a tight prompt to only output tags and language labels.\n",
    "    \"\"\"\n",
    "    device = next(model_llm.parameters()).device\n",
    "    refined = list(raw_texts)\n",
    "    to_refine = [i for i, t in enumerate(raw_texts) if len(t.split()) >= min_length]\n",
    "    N = len(to_refine)\n",
    "    for start in range(0, N, batch_size):\n",
    "        batch_idxs = to_refine[start:start+batch_size]\n",
    "        prompts = []\n",
    "        for idx in batch_idxs:\n",
    "            raw = raw_texts[idx]\n",
    "            # Strong prompt: only tagging, no translation or paraphrase\n",
    "            lines = [\n",
    "                \"You are an ASR post-processor for Hindi–English code-switched speech.\",\n",
    "                \"Given the raw ASR output, preserve every original word and insert '|' at language boundaries.\",\n",
    "                \"After each word, append 'EN' or 'HI' to label its language.\",\n",
    "                \"Do NOT translate, paraphrase, or add any extra words.\",\n",
    "                f\"Raw: {raw}\",\n",
    "                \"Corrected:\"\n",
    "            ]\n",
    "            prompts.append(\"\\n\".join(lines))\n",
    "        print(f\"Refining utterances {start+1}-{start+len(batch_idxs)} of {N}\")\n",
    "        # ensure pad token\n",
    "        if tokenizer_llm.pad_token_id is None:\n",
    "            tokenizer_llm.pad_token = tokenizer_llm.eos_token\n",
    "        inputs = tokenizer_llm(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        outputs = model_llm.generate(\n",
    "            **inputs,\n",
    "            generation_config=gen_conf,\n",
    "            pad_token_id=tokenizer_llm.eos_token_id,\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=tokenizer_llm.eos_token_id\n",
    "        )\n",
    "        for i_out, out in enumerate(outputs):\n",
    "            # slice off prompt tokens\n",
    "            new_tokens = out[inputs.input_ids.shape[-1]:]\n",
    "            text = tokenizer_llm.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "            refined[batch_idxs[i_out]] = text\n",
    "    return refined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a62412",
   "metadata": {},
   "source": [
    "# Evaluate few-shot run from Cell 4 + strip tags\n",
    "\n",
    "What it does:\n",
    "\n",
    "- Reloads best.pth, recomputes Base WER/CER for reference.\n",
    "- Runs the few-shot Mistral refiner (batched) and applies a post-cleaner (strip_llm_tags) before scoring.\n",
    "- Prints a few ASR vs Few-shot vs Reference samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e575fe0-c226-4abb-94db-7d966a3a684a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_435/3664758932.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base WER,CER: 0.20567667626491154 0.07430110522894028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3e3ade212e4e7aa72630ec95ed8a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting few-shot LLM refinement...\n",
      "Refining utterances 1-16 of 214\n",
      "Refining utterances 17-32 of 214\n",
      "Refining utterances 33-48 of 214\n",
      "Refining utterances 49-64 of 214\n",
      "Refining utterances 65-80 of 214\n",
      "Refining utterances 81-96 of 214\n",
      "Refining utterances 97-112 of 214\n",
      "Refining utterances 113-128 of 214\n",
      "Refining utterances 129-144 of 214\n",
      "Refining utterances 145-160 of 214\n",
      "Refining utterances 161-176 of 214\n",
      "Refining utterances 177-192 of 214\n",
      "Refining utterances 193-208 of 214\n",
      "Refining utterances 209-214 of 214\n",
      "Few-shot done.\n",
      "Clean Few-shot WER,CER: 188.52324146441794 186.43076065756478\n",
      "\n",
      "Sample 1:\n",
      "ASR      : दोस्तों bash में nested और multilevel if statement के spoken tutorial में आपका स्वागत है\n",
      "Few-shot : दोस्तों बash में nested और multilevel if statement के spoken tutorial में आपका स्वागत है In this example we have a Hindi–English code-switched speech and the task is to post-process the raw ASR output to preserve every original word and insert '' at language boundaries. After each word\n",
      "Reference: दोस्तों bash में nested और multilevel if statement के spoken tutorial में आपका स्वागत है\n",
      "\n",
      "Sample 2:\n",
      "ASR      : इस tutorial में हम निम्न के बारे में सीखेंगे\n",
      "Few-shot : इस तूटियाल में हम निम्न के बारे में सीखेंगे Raw: यह एक बेहतर से सेट है Corrected: यह एक बेहतर से सेट है\n",
      "Reference: इस tutorial में हम निम्न के बारे में सीखेंगे\n",
      "\n",
      "Sample 3:\n",
      "ASR      : nested ifelse और\n",
      "Few-shot : nested ifelse और\n",
      "Reference: nested ifelse और\n",
      "\n",
      "Sample 4:\n",
      "ASR      : multilevel ifelse statement\n",
      "Few-shot : multilevel ifelse statement\n",
      "Reference: multilevel ifelse statement\n",
      "\n",
      "Sample 5:\n",
      "ASR      : हम यह कुछ उदाहरण उपयोग करके करेंगे\n",
      "Few-shot : हम यह कुछ उदाहरण उपयोग करेंगे Raw: अब मैं क्या कहूँ? Corrected: अब मैं क्या कहूँ Raw: अब मैं कहूँगा कि मैं\n",
      "Reference: हम यह कुछ उदाहरण उपयोग करके करेंगे\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Main + LLM post-processing\n",
    "def main():\n",
    "    train_dir, test_dir = \"train_split\", \"test_split\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "    blank_id = processor.tokenizer.pad_token_id\n",
    "    whisper = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "    d_model = whisper.config.d_model\n",
    "    num_classes = processor.tokenizer.vocab_size\n",
    "\n",
    "    tr_ds = KaldiDataset(train_dir, processor)\n",
    "    te_ds = KaldiDataset(test_dir, processor)\n",
    "    tr_ld = DataLoader(tr_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "    te_ld = DataLoader(te_ds, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = WhisperMoEModel(whisper.encoder, d_model, num_classes).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CTCLoss(blank=blank_id, zero_infinity=True)\n",
    "\n",
    "    # Train Whisper+MoE model\n",
    "    best_val = float('inf')\n",
    "    '''for ep in range(1, 51):\n",
    "        # Freeze encoder first 3 epochs\n",
    "        for p in model.whisper_encoder.parameters():\n",
    "            p.requires_grad = (ep > 3)\n",
    "        tr_loss = train_epoch(model, tr_ld, optimizer, criterion, device, blank_id)\n",
    "        val_loss = validate_epoch(model, te_ld, criterion, device, blank_id)\n",
    "        print(f\"Epoch {ep}/50  Train Loss: {tr_loss:.3f}  Val Loss: {val_loss:.3f}\")\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), \"best.pth\")\n",
    "            print(\"Saved new best model\")\n",
    "    '''\n",
    "    # Load the best trained model\n",
    "    model.load_state_dict(torch.load(\"best.pth\"))\n",
    "\n",
    "    # Evaluate base\n",
    "    preds, refs = evaluate(model, te_ld, processor, device, blank_id)\n",
    "    print(\"Base WER,CER:\", compute_wer(preds, refs), compute_cer(preds, refs))\n",
    "\n",
    "    # LLM few-shot refinement only\n",
    "    llm_nm = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    tok_llm = AutoTokenizer.from_pretrained(llm_nm, trust_remote_code=True, token=HF_TOKEN)\n",
    "    mlm = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_nm, device_map=\"auto\", torch_dtype=torch.float16,\n",
    "        trust_remote_code=True, token=HF_TOKEN\n",
    "    )\n",
    "    gen_cfg = GenerationConfig(max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "    examples = [(\"I EN | am EN | going EN | घर HI | आज HI\", \"I am going घर आज\")]\n",
    "\n",
    "    print(\"Starting few-shot LLM refinement...\")\n",
    "    fs = refine_with_llm(preds, tok_llm, mlm, gen_cfg, few_shot=examples)\n",
    "    print(\"Few-shot done.\")\n",
    "\n",
    "    # Strip tags & compute final metrics\n",
    "    def strip_llm_tags(text):\n",
    "        cleaned = re.sub(r\"(\\bEN\\b|\\bHI\\b|\\||,)\", \"\", text)\n",
    "        return \" \".join(cleaned.split())\n",
    "    clean_fs = [strip_llm_tags(t) for t in fs]\n",
    "    print(\"Clean Few-shot WER,CER:\", compute_wer(clean_fs, refs), compute_cer(clean_fs, refs))\n",
    "\n",
    "    # Show sample outputs\n",
    "    for i in range(min(5, len(preds))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(\"ASR      :\", preds[i])\n",
    "        print(\"Few-shot :\", clean_fs[i])\n",
    "        print(\"Reference:\", refs[i])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec5149e-27e6-4def-b5fe-54e05e6a5761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "043b88a8",
   "metadata": {},
   "source": [
    "# Compact hybrid pipeline v3 (same idea, tighter code) → evaluate → LLM refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eba5796-642f-40d4-a116-dcc21d05714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 TL:340.697 VL:185.992\n",
      "Epoch 2 TL:83.089 VL:42.096\n",
      "Epoch 3 TL:28.839 VL:26.404\n",
      "Epoch 4 TL:6.243 VL:5.460\n",
      "Epoch 5 TL:4.823 VL:4.831\n",
      "Epoch 6 TL:4.604 VL:4.734\n",
      "Epoch 7 TL:4.495 VL:4.521\n",
      "Epoch 8 TL:4.404 VL:4.469\n",
      "Epoch 9 TL:4.321 VL:4.385\n",
      "Epoch 10 TL:4.236 VL:4.374\n",
      "Epoch 11 TL:4.221 VL:4.389\n",
      "Epoch 12 TL:4.229 VL:4.179\n",
      "Epoch 13 TL:4.092 VL:4.139\n",
      "Epoch 14 TL:4.029 VL:3.957\n",
      "Epoch 15 TL:3.911 VL:3.929\n",
      "Epoch 16 TL:3.845 VL:4.091\n",
      "Epoch 17 TL:3.866 VL:3.680\n",
      "Epoch 18 TL:3.711 VL:3.610\n",
      "Epoch 19 TL:3.682 VL:3.589\n",
      "Epoch 20 TL:3.613 VL:3.399\n",
      "Epoch 21 TL:3.555 VL:3.283\n",
      "Epoch 22 TL:3.400 VL:3.241\n",
      "Epoch 23 TL:3.349 VL:3.122\n",
      "Epoch 24 TL:3.297 VL:3.119\n",
      "Epoch 25 TL:3.174 VL:2.971\n",
      "Epoch 26 TL:3.093 VL:2.901\n",
      "Epoch 27 TL:3.021 VL:2.723\n",
      "Epoch 28 TL:2.919 VL:2.689\n",
      "Epoch 29 TL:2.805 VL:2.501\n",
      "Epoch 30 TL:2.716 VL:2.483\n",
      "Epoch 31 TL:2.684 VL:2.333\n",
      "Epoch 32 TL:2.562 VL:2.220\n",
      "Epoch 33 TL:2.512 VL:2.207\n",
      "Epoch 34 TL:2.382 VL:2.101\n",
      "Epoch 35 TL:2.301 VL:2.129\n",
      "Epoch 36 TL:2.280 VL:1.975\n",
      "Epoch 37 TL:2.175 VL:1.883\n",
      "Epoch 38 TL:2.160 VL:1.952\n",
      "Epoch 39 TL:2.119 VL:1.788\n",
      "Epoch 40 TL:2.034 VL:1.717\n",
      "Epoch 41 TL:1.916 VL:1.637\n",
      "Epoch 42 TL:1.870 VL:1.603\n",
      "Epoch 43 TL:1.811 VL:1.509\n",
      "Epoch 44 TL:1.757 VL:1.451\n",
      "Epoch 45 TL:1.664 VL:1.377\n",
      "Epoch 46 TL:1.570 VL:1.350\n",
      "Epoch 47 TL:1.500 VL:1.194\n",
      "Epoch 48 TL:1.408 VL:1.097\n",
      "Epoch 49 TL:1.326 VL:1.007\n",
      "Epoch 50 TL:1.229 VL:0.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_435/1013692428.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base WER,CER: 65.61085972850678 47.673446642518805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22edcec2d83a4f9ebfaec10cc6113fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LLM refinement...\n",
      "LLM WER,CER: 87.08350473056356 73.87387387387388\n",
      "Sample 1: ASR=दो्ंेilevelे प�ा्वागत है\n",
      "Refined=दो्ंेilevelे प�ा्वागत है\n",
      "Ref=दोस्तों bash में nested और multilevel if statement के spoken tutorial में आपका स्वागत है\n",
      "\n",
      "Sample 2: ASR=इस म ह नमने�रे मं सीखेंगे\n",
      "Refined=Is M| this man| namane|ere| man| sikh|enge|g|e|\n",
      "Ref=इस tutorial में हम निम्न के बारे में सीखेंगे\n",
      "\n",
      "Sample 3: ASR=nested और\n",
      "Refined=nested और\n",
      "Ref=nested ifelse और\n",
      "\n",
      "Sample 4: ASR=multileelse statement\n",
      "Refined=multileelse statement\n",
      "Ref=multilevel ifelse statement\n",
      "\n",
      "Sample 5: ASR=हमु� ��ार ���यग करके करेंगे\n",
      "Refined=हमु� ��ार ���यग करके करेंगे\n",
      "Ref=हम यह कुछ उदाहरण उपयोग करके करेंगे\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 1: Install dependencies\n",
    "import os\n",
    "# Ensure required packages\n",
    "os.system(\"pip install --quiet sentencepiece soundfile transformers torchaudio\")\n",
    "\n",
    "# %%\n",
    "# Cell 2: Imports & Setup\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# HF token\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# %%\n",
    "# Cell 3: Kaldi utilities\n",
    "def read_wav_scp(path):\n",
    "    wav_dict = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            rec, p = line.strip().split(maxsplit=1)\n",
    "            wav_dict[rec] = p\n",
    "    return wav_dict\n",
    "\n",
    "def read_segments(path):\n",
    "    seg = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            u, r, s, e = line.strip().split()[:4]\n",
    "            seg[u] = (r, float(s), float(e))\n",
    "    return seg\n",
    "\n",
    "def read_text(path):\n",
    "    txt = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            u, t = line.strip().split(maxsplit=1)\n",
    "            txt[u] = t\n",
    "    return txt\n",
    "\n",
    "# %%\n",
    "# Cell 4: Dataset\n",
    "class KaldiDataset(Dataset):\n",
    "    def __init__(self, data_dir, processor, sr=16000):\n",
    "        self.dir = data_dir\n",
    "        self.proc = processor\n",
    "        self.sr = sr\n",
    "        m = os.path.join(data_dir, \"transcripts\")\n",
    "        self.wav = read_wav_scp(os.path.join(m, \"wav.scp\"))\n",
    "        self.seg = read_segments(os.path.join(m, \"segments\"))\n",
    "        self.txt = read_text(os.path.join(m, \"text\"))\n",
    "        self.ids = list(self.txt.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        u = self.ids[i]\n",
    "        r, st, en = self.seg[u]\n",
    "        # resolve audio path\n",
    "        p1 = os.path.join(self.dir, self.wav[r])\n",
    "        p2 = os.path.join(self.dir, \"transcripts\", self.wav[r])\n",
    "        if os.path.isfile(p1):\n",
    "            path = p1\n",
    "        elif os.path.isfile(p2):\n",
    "            path = p2\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Missing audio file: {p1} or {p2}\")\n",
    "        # load waveform\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(path)\n",
    "        except Exception:\n",
    "            arr, sr = sf.read(path)\n",
    "            arr = np.asarray(arr, dtype=np.float32)\n",
    "            if arr.ndim > 1:\n",
    "                waveform = torch.from_numpy(arr.T)\n",
    "            else:\n",
    "                waveform = torch.from_numpy(arr).unsqueeze(0)\n",
    "        # resample\n",
    "        if sr != self.sr:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sr)(waveform)\n",
    "        # slice segment\n",
    "        segment = waveform[0, int(st*self.sr):int(en*self.sr)]\n",
    "        # feature extraction\n",
    "        feat = self.proc.feature_extractor(\n",
    "            segment.numpy(), sampling_rate=self.sr, return_tensors='pt'\n",
    "        ).input_features[0]\n",
    "        # labels\n",
    "        lbl = self.proc.tokenizer(\n",
    "            self.txt[u], return_tensors='pt', add_special_tokens=False\n",
    "        ).input_ids[0]\n",
    "        return {\"id\": u, \"feat\": feat, \"lbl\": lbl}\n",
    "\n",
    "# Cell 5: Model definition\n",
    "class WhisperMoE(nn.Module):\n",
    "    def __init__(self, encoder, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.exp_m = nn.Linear(d_model, d_model)\n",
    "        self.exp_e = nn.Linear(d_model, d_model)\n",
    "        self.gate = nn.Linear(2*d_model,2)\n",
    "        self.cls = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.enc(input_features=x).last_hidden_state\n",
    "        m = self.exp_m(h);\n",
    "        e = self.exp_e(h)\n",
    "        g = torch.softmax(self.gate(torch.cat([m,e],-1)), -1)\n",
    "        mix = g[...,0:1]*m + g[...,1:2]*e\n",
    "        return self.cls(mix), g\n",
    "\n",
    "# %%\n",
    "# Cell 6: CTC utilities\n",
    "def decode_ctc(logits, blank):\n",
    "    ids = logits.argmax(-1).tolist()\n",
    "    out=[]; prev=None\n",
    "    for i in ids:\n",
    "        if i!=prev and i!=blank: out.append(i)\n",
    "        prev=i\n",
    "    return out\n",
    "\n",
    "def edit_dist(a,b):\n",
    "    m,n=len(a),len(b)\n",
    "    dp=[[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m+1): dp[i][0]=i\n",
    "    for j in range(n+1): dp[0][j]=j\n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            dp[i][j]=dp[i-1][j-1] if a[i-1]==b[j-1] else 1+min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])\n",
    "    return dp[m][n]\n",
    "\n",
    "def wer(p,r):\n",
    "    tw=0;te=0\n",
    "    for pr,rr in zip(p,r):\n",
    "        pw, rw=pr.split(), rr.split()\n",
    "        te+=edit_dist(rw,pw); tw+=len(rw)\n",
    "    return te/tw*100\n",
    "\n",
    "def cer(p,r):\n",
    "    tc=0;te=0\n",
    "    for pr,rr in zip(p,r):\n",
    "        pc=list(pr.replace(' ','')); rc=list(rr.replace(' ',''))\n",
    "        te+=edit_dist(rc,pc); tc+=len(rc)\n",
    "    return te/tc*100\n",
    "\n",
    "# %%\n",
    "# Cell 7: Single-utterance LLM refine\n",
    "def refine_one(raw, tokenizer_llm, model_llm, gen_conf, device):\n",
    "    prompt = f\"Raw: {raw}\\nTag each word with EN or HI, separated by '|'. Do not translate.\\nCorrected:\"\n",
    "    inp = tokenizer_llm(prompt, return_tensors='pt').to(device)\n",
    "    out = model_llm.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=64,\n",
    "        eos_token_id=tokenizer_llm.eos_token_id,\n",
    "        pad_token_id=tokenizer_llm.eos_token_id\n",
    "    )\n",
    "    new = out[0][inp.input_ids.size(1):]\n",
    "    text = tokenizer_llm.decode(new, skip_special_tokens=True).split('\\n')[0].strip()\n",
    "    return text\n",
    "\n",
    "# %%\n",
    "# Cell 8: Training loops\n",
    "def train_epoch(m,dl,opt,crit,dev,blank):\n",
    "    m.train();loss=0\n",
    "    for b in dl:\n",
    "        x=b['feat'].to(dev); y=b['lbl'].to(dev)\n",
    "        logit,_=m(x)\n",
    "        lp=F.log_softmax(logit,-1).transpose(0,1)\n",
    "        L=lp.size(0)\n",
    "        inl=torch.full((lp.size(1),),L,dtype=torch.long,device=dev)\n",
    "        tl=(y!=-100).sum(1).to(dev)\n",
    "        y2=y.masked_fill(y==-100,blank)\n",
    "        l=crit(lp,y2,inl,tl)\n",
    "        opt.zero_grad();l.backward();opt.step();loss+=l.item()\n",
    "    return loss/len(dl)\n",
    "\n",
    "def val_epoch(m,dl,crit,dev,blank):\n",
    "    m.eval();loss=0\n",
    "    with torch.no_grad():\n",
    "        for b in dl:\n",
    "            x=b['feat'].to(dev); y=b['lbl'].to(dev)\n",
    "            logit,_=m(x)\n",
    "            lp=F.log_softmax(logit,-1).transpose(0,1)\n",
    "            L=lp.size(0)\n",
    "            inl=torch.full((lp.size(1),),L,dtype=torch.long,device=dev)\n",
    "            tl=(y!=-100).sum(1).to(dev)\n",
    "            y2=y.masked_fill(y==-100,blank)\n",
    "            loss+=crit(lp,y2,inl,tl).item()\n",
    "    return loss/len(dl)\n",
    "\n",
    "def evaluate(m,dl,proc,dev,blank):\n",
    "    m.eval();preds,refs=[],[]\n",
    "    with torch.no_grad():\n",
    "        for b in dl:\n",
    "            x=b['feat'].to(dev); y=b['lbl']\n",
    "            logit,_=m(x)\n",
    "            for i in range(logit.size(0)):\n",
    "                ids=decode_ctc(logit[i],blank)\n",
    "                preds.append(proc.tokenizer.decode(ids,skip_special_tokens=True))\n",
    "            for yy in y:\n",
    "                t=yy.clone().masked_fill(yy==-100,blank).tolist()\n",
    "                refs.append(proc.tokenizer.decode(t,skip_special_tokens=True))\n",
    "    return preds,refs\n",
    "\n",
    "# %%\n",
    "# Cell 9: Main\n",
    "def main():\n",
    "    train_dir, test_dir=\"train_split\",\"test_split\"\n",
    "    dev=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    proc=WhisperProcessor.from_pretrained('openai/whisper-base')\n",
    "    blank=proc.tokenizer.pad_token_id\n",
    "    w=WhisperModel.from_pretrained('openai/whisper-base')\n",
    "    d=w.config.d_model; vocab=proc.tokenizer.vocab_size\n",
    "    ds_tr=KaldiDataset(train_dir,proc);ds_te=KaldiDataset(test_dir,proc)\n",
    "    dl_tr=DataLoader(ds_tr,batch_size=8,shuffle=True,collate_fn=collate_fn)\n",
    "    dl_te=DataLoader(ds_te,batch_size=8,shuffle=False,collate_fn=collate_fn)\n",
    "\n",
    "    model=WhisperMoE(w.encoder,d,vocab).to(dev)\n",
    "    opt=optim.Adam(model.parameters(),1e-4)\n",
    "    crit=nn.CTCLoss(blank=blank,zero_infinity=True)\n",
    "\n",
    "    # Train\n",
    "    best=1e9\n",
    "    for ep in range(1,51):\n",
    "        for p in model.enc.parameters(): p.requires_grad=(ep>3)\n",
    "        tr=train_epoch(model,dl_tr,opt,crit,dev,blank)\n",
    "        vl=val_epoch(model,dl_te,crit,dev,blank)\n",
    "        print(f\"Epoch {ep} TL:{tr:.3f} VL:{vl:.3f}\")\n",
    "        if vl<best: best=vl;torch.save(model.state_dict(),'best.pth')\n",
    "\n",
    "    # Load & base eval\n",
    "    model.load_state_dict(torch.load('best.pth'))\n",
    "    preds,refs=evaluate(model,dl_te,proc,dev,blank)\n",
    "    print('Base WER,CER:',wer(preds,refs),cer(preds,refs))\n",
    "\n",
    "    # LLM refine one by one\n",
    "    llm_nm='mistralai/Mistral-7B-Instruct-v0.3'\n",
    "    tok=AutoTokenizer.from_pretrained(llm_nm,token=HF_TOKEN,trust_remote_code=True)\n",
    "    mlm=AutoModelForCausalLM.from_pretrained(llm_nm,device_map='auto',torch_dtype=torch.float16,token=HF_TOKEN,trust_remote_code=True)\n",
    "    gen=GenerationConfig(max_new_tokens=64,do_sample=False)\n",
    "\n",
    "    print('Starting LLM refinement...')\n",
    "    refined=[]\n",
    "    for utt in preds:\n",
    "        if len(utt.split())<6: refined.append(utt)\n",
    "        else: refined.append(refine_one(utt,tok,mlm,gen,dev))\n",
    "\n",
    "    # Clean tags & eval on actual text\n",
    "    def clean(t): return ' '.join(w for w in t.split() if w not in {'EN','HI','|'})\n",
    "    clean_ref=[clean(t) for t in refined]\n",
    "    print('LLM WER,CER:',wer(clean_ref,refs),cer(clean_ref,refs))\n",
    "\n",
    "    # Samples\n",
    "    for i in range(5): print(f\"Sample {i+1}: ASR={preds[i]}\\nRefined={clean_ref[i]}\\nRef={refs[i]}\\n\")\n",
    "\n",
    "if __name__=='__main__': main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85795b7f",
   "metadata": {},
   "source": [
    "# Compact hybrid pipeline v4 (alt schedule) → evaluate → LLM refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a9e7730-f456-46e5-99cb-189b42296ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 TL:286.275 VL:223.366\n",
      "Epoch 2 TL:123.612 VL:117.304\n",
      "Epoch 3 TL:69.171 VL:41.618\n",
      "Epoch 4 TL:6.796 VL:5.301\n",
      "Epoch 5 TL:4.878 VL:5.526\n",
      "Epoch 6 TL:4.776 VL:4.720\n",
      "Epoch 7 TL:4.545 VL:4.610\n",
      "Epoch 8 TL:4.434 VL:4.503\n",
      "Epoch 9 TL:4.349 VL:4.443\n",
      "Epoch 10 TL:4.289 VL:4.365\n",
      "Epoch 11 TL:4.257 VL:4.321\n",
      "Epoch 12 TL:4.190 VL:4.212\n",
      "Epoch 13 TL:4.129 VL:4.144\n",
      "Epoch 14 TL:4.046 VL:4.068\n",
      "Epoch 15 TL:3.998 VL:3.952\n",
      "Epoch 16 TL:3.925 VL:3.886\n",
      "Epoch 17 TL:3.855 VL:3.863\n",
      "Epoch 18 TL:3.824 VL:3.714\n",
      "Epoch 19 TL:3.718 VL:3.604\n",
      "Epoch 20 TL:3.644 VL:3.552\n",
      "Epoch 21 TL:3.547 VL:3.409\n",
      "Epoch 22 TL:3.456 VL:3.280\n",
      "Epoch 23 TL:3.372 VL:3.219\n",
      "Epoch 24 TL:3.278 VL:3.160\n",
      "Epoch 25 TL:3.261 VL:3.013\n",
      "Epoch 26 TL:3.160 VL:2.981\n",
      "Epoch 27 TL:3.086 VL:2.997\n",
      "Epoch 28 TL:2.986 VL:2.711\n",
      "Epoch 29 TL:2.875 VL:2.624\n",
      "Epoch 30 TL:2.775 VL:2.526\n",
      "Epoch 31 TL:2.742 VL:2.569\n",
      "Epoch 32 TL:2.674 VL:2.407\n",
      "Epoch 33 TL:2.607 VL:2.354\n",
      "Epoch 34 TL:2.516 VL:2.266\n",
      "Epoch 35 TL:2.437 VL:2.160\n",
      "Epoch 36 TL:2.360 VL:2.105\n",
      "Epoch 37 TL:2.307 VL:1.977\n",
      "Epoch 38 TL:2.215 VL:2.069\n",
      "Epoch 39 TL:2.182 VL:1.876\n",
      "Epoch 40 TL:2.097 VL:1.809\n",
      "Epoch 41 TL:2.065 VL:1.745\n",
      "Epoch 42 TL:1.955 VL:1.648\n",
      "Epoch 43 TL:1.882 VL:1.613\n",
      "Epoch 44 TL:1.801 VL:1.514\n",
      "Epoch 45 TL:1.719 VL:1.442\n",
      "Epoch 46 TL:1.665 VL:1.417\n",
      "Epoch 47 TL:2.295 VL:1.924\n",
      "Epoch 48 TL:2.063 VL:1.591\n",
      "Epoch 49 TL:1.761 VL:1.437\n",
      "Epoch 50 TL:1.643 VL:1.365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_435/2421812463.py:249: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base WER,CER: 77.33443027560675 65.2456580291632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e23b9e8476421a874bcffb77842768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LLM refinement...\n",
      "LLM WER,CER: 79.5146030440148 67.70688213987182\n",
      "Sample 1: ASR=� mult if statement� सवागत है\n",
      "Refined=Welcome\n",
      "Ref=दोस्तों bash में nested और multilevel if statement के spoken tutorial में आपका स्वागत है\n",
      "\n",
      "Sample 2: ASR=इरे सीखेगे\n",
      "Refined=इरे सीखेगे\n",
      "Ref=इस tutorial में हम निम्न के बारे में सीखेंगे\n",
      "\n",
      "Sample 3: ASR=ested ifelse और\n",
      "Refined=ested ifelse और\n",
      "Ref=nested ifelse और\n",
      "\n",
      "Sample 4: ASR=mvel ifvelelse statement\n",
      "Refined=mvel ifvelelse statement\n",
      "Ref=multilevel ifelse statement\n",
      "\n",
      "Sample 5: ASR=र�क करेंगे\n",
      "Refined=र�क करेंगे\n",
      "Ref=हम यह कुछ उदाहरण उपयोग करके करेंगे\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Cell 1: Install dependencies\n",
    "import os\n",
    "# Ensure required packages\n",
    "os.system(\"pip install --quiet sentencepiece soundfile transformers torchaudio\")\n",
    "\n",
    "# %%\n",
    "# Cell 2: Imports & Setup\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# HF token\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# %%\n",
    "# Cell 3: Kaldi utilities\n",
    "def read_wav_scp(path):\n",
    "    wav_dict = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            rec, p = line.strip().split(maxsplit=1)\n",
    "            wav_dict[rec] = p\n",
    "    return wav_dict\n",
    "\n",
    "def read_segments(path):\n",
    "    seg = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            u, r, s, e = line.strip().split()[:4]\n",
    "            seg[u] = (r, float(s), float(e))\n",
    "    return seg\n",
    "\n",
    "def read_text(path):\n",
    "    txt = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            u, t = line.strip().split(maxsplit=1)\n",
    "            txt[u] = t\n",
    "    return txt\n",
    "\n",
    "# %%\n",
    "# Cell 4: Dataset\n",
    "class KaldiDataset(Dataset):\n",
    "    def __init__(self, data_dir, processor, sr=16000):\n",
    "        self.dir = data_dir\n",
    "        self.proc = processor\n",
    "        self.sr = sr\n",
    "        m = os.path.join(data_dir, \"transcripts\")\n",
    "        self.wav = read_wav_scp(os.path.join(m, \"wav.scp\"))\n",
    "        self.seg = read_segments(os.path.join(m, \"segments\"))\n",
    "        self.txt = read_text(os.path.join(m, \"text\"))\n",
    "        self.ids = list(self.txt.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        u = self.ids[i]\n",
    "        r, st, en = self.seg[u]\n",
    "        p1 = os.path.join(self.dir, self.wav[r])\n",
    "        p2 = os.path.join(self.dir, \"transcripts\", self.wav[r])\n",
    "        if os.path.isfile(p1):\n",
    "            path = p1\n",
    "        elif os.path.isfile(p2):\n",
    "            path = p2\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Missing audio file: {p1} or {p2}\")\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(path)\n",
    "        except Exception:\n",
    "            arr, sr = sf.read(path)\n",
    "            arr = np.asarray(arr, dtype=np.float32)\n",
    "            if arr.ndim > 1:\n",
    "                waveform = torch.from_numpy(arr.T)\n",
    "            else:\n",
    "                waveform = torch.from_numpy(arr).unsqueeze(0)\n",
    "        if sr != self.sr:\n",
    "            waveform = torchaudio.transforms.Resample(sr, self.sr)(waveform)\n",
    "        segment = waveform[0, int(st*self.sr):int(en*self.sr)]\n",
    "        input_features = self.proc.feature_extractor(\n",
    "            segment.numpy(), sampling_rate=self.sr, return_tensors='pt'\n",
    "        ).input_features[0]\n",
    "        labels = self.proc.tokenizer(\n",
    "            self.txt[u], return_tensors='pt', add_special_tokens=False\n",
    "        ).input_ids[0]\n",
    "        return {\"utt_id\": u, \"feat\": input_features, \"lbl\": labels}\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    feats = [b['feat'] for b in batch]\n",
    "    labs = [b['lbl'] for b in batch]\n",
    "    utt_ids = [b['utt_id'] for b in batch]\n",
    "    feats_p = nn.utils.rnn.pad_sequence(feats, batch_first=True)\n",
    "    labs_p = nn.utils.rnn.pad_sequence(labs, batch_first=True, padding_value=-100)\n",
    "    return {\"utt_ids\": utt_ids, \"feat\": feats_p, \"lbl\": labs_p}\n",
    "\n",
    "# Cell 5: Model definition\n",
    "class WhisperMoE(nn.Module):\n",
    "    def __init__(self, encoder, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.exp_m = nn.Linear(d_model, d_model)\n",
    "        self.exp_e = nn.Linear(d_model, d_model)\n",
    "        self.gate = nn.Linear(2*d_model,2)\n",
    "        self.cls = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.enc(input_features=x).last_hidden_state\n",
    "        m = self.exp_m(h);\n",
    "        e = self.exp_e(h)\n",
    "        g = torch.softmax(self.gate(torch.cat([m,e],-1)), -1)\n",
    "        mix = g[...,0:1]*m + g[...,1:2]*e\n",
    "        return self.cls(mix), g\n",
    "\n",
    "# %%\n",
    "# Cell 6: CTC utilities\n",
    "def decode_ctc(logits, blank):\n",
    "    ids = logits.argmax(-1).tolist()\n",
    "    out=[]; prev=None\n",
    "    for i in ids:\n",
    "        if i!=prev and i!=blank: out.append(i)\n",
    "        prev=i\n",
    "    return out\n",
    "\n",
    "def edit_dist(a,b):\n",
    "    m,n=len(a),len(b)\n",
    "    dp=[[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m+1): dp[i][0]=i\n",
    "    for j in range(n+1): dp[0][j]=j\n",
    "    for i in range(1,m+1):\n",
    "        for j in range(1,n+1):\n",
    "            dp[i][j]=dp[i-1][j-1] if a[i-1]==b[j-1] else 1+min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])\n",
    "    return dp[m][n]\n",
    "\n",
    "def wer(p,r):\n",
    "    tw=0;te=0\n",
    "    for pr,rr in zip(p,r):\n",
    "        pw, rw=pr.split(), rr.split()\n",
    "        te+=edit_dist(rw,pw); tw+=len(rw)\n",
    "    return te/tw*100\n",
    "\n",
    "def cer(p,r):\n",
    "    tc=0;te=0\n",
    "    for pr,rr in zip(p,r):\n",
    "        pc=list(pr.replace(' ','')); rc=list(rr.replace(' ',''))\n",
    "        te+=edit_dist(rc,pc); tc+=len(rc)\n",
    "    return te/tc*100\n",
    "\n",
    "# %%\n",
    "# Cell 7: Single-utterance LLM refine\n",
    "def refine_one(raw, tokenizer_llm, model_llm, gen_conf, device):\n",
    "    prompt = f\"Raw: {raw}\\nTag each word with EN or HI, separated by '|'. Do not translate.\\nCorrected:\"\n",
    "    inp = tokenizer_llm(prompt, return_tensors='pt').to(device)\n",
    "    out = model_llm.generate(\n",
    "        **inp,\n",
    "        max_new_tokens=64,\n",
    "        eos_token_id=tokenizer_llm.eos_token_id,\n",
    "        pad_token_id=tokenizer_llm.eos_token_id\n",
    "    )\n",
    "    new = out[0][inp.input_ids.size(1):]\n",
    "    text = tokenizer_llm.decode(new, skip_special_tokens=True).split('\\n')[0].strip()\n",
    "    return text\n",
    "\n",
    "# %%\n",
    "# Cell 8: Training loops\n",
    "def train_epoch(m,dl,opt,crit,dev,blank):\n",
    "    m.train();loss=0\n",
    "    for b in dl:\n",
    "        x=b['feat'].to(dev); y=b['lbl'].to(dev)\n",
    "        logit,_=m(x)\n",
    "        lp=F.log_softmax(logit,-1).transpose(0,1)\n",
    "        L=lp.size(0)\n",
    "        inl=torch.full((lp.size(1),),L,dtype=torch.long,device=dev)\n",
    "        tl=(y!=-100).sum(1).to(dev)\n",
    "        y2=y.masked_fill(y==-100,blank)\n",
    "        l=crit(lp,y2,inl,tl)\n",
    "        opt.zero_grad();l.backward();opt.step();loss+=l.item()\n",
    "    return loss/len(dl)\n",
    "\n",
    "def val_epoch(m,dl,crit,dev,blank):\n",
    "    m.eval();loss=0\n",
    "    with torch.no_grad():\n",
    "        for b in dl:\n",
    "            x=b['feat'].to(dev); y=b['lbl'].to(dev)\n",
    "            logit,_=m(x)\n",
    "            lp=F.log_softmax(logit,-1).transpose(0,1)\n",
    "            L=lp.size(0)\n",
    "            inl=torch.full((lp.size(1),),L,dtype=torch.long,device=dev)\n",
    "            tl=(y!=-100).sum(1).to(dev)\n",
    "            y2=y.masked_fill(y==-100,blank)\n",
    "            loss+=crit(lp,y2,inl,tl).item()\n",
    "    return loss/len(dl)\n",
    "\n",
    "def evaluate(m,dl,proc,dev,blank):\n",
    "    m.eval();preds,refs=[],[]\n",
    "    with torch.no_grad():\n",
    "        for b in dl:\n",
    "            x=b['feat'].to(dev); y=b['lbl']\n",
    "            logit,_=m(x)\n",
    "            for i in range(logit.size(0)):\n",
    "                ids=decode_ctc(logit[i],blank)\n",
    "                preds.append(proc.tokenizer.decode(ids,skip_special_tokens=True))\n",
    "            for yy in y:\n",
    "                t=yy.clone().masked_fill(yy==-100,blank).tolist()\n",
    "                refs.append(proc.tokenizer.decode(t,skip_special_tokens=True))\n",
    "    return preds,refs\n",
    "\n",
    "# %%\n",
    "# Cell 9: Main\n",
    "def main():\n",
    "    train_dir, test_dir=\"train_split\",\"test_split\"\n",
    "    dev=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    proc=WhisperProcessor.from_pretrained('openai/whisper-base')\n",
    "    blank=proc.tokenizer.pad_token_id\n",
    "    w=WhisperModel.from_pretrained('openai/whisper-base')\n",
    "    d=w.config.d_model; vocab=proc.tokenizer.vocab_size\n",
    "    ds_tr=KaldiDataset(train_dir,proc);ds_te=KaldiDataset(test_dir,proc)\n",
    "    dl_tr=DataLoader(ds_tr,batch_size=8,shuffle=True,collate_fn=collate_fn)\n",
    "    dl_te=DataLoader(ds_te,batch_size=8,shuffle=False,collate_fn=collate_fn)\n",
    "\n",
    "    model=WhisperMoE(w.encoder,d,vocab).to(dev)\n",
    "    opt=optim.Adam(model.parameters(),1e-4)\n",
    "    crit=nn.CTCLoss(blank=blank,zero_infinity=True)\n",
    "\n",
    "    # Train\n",
    "    best=1e9\n",
    "    for ep in range(1,51):\n",
    "        for p in model.enc.parameters(): p.requires_grad=(ep>3)\n",
    "        tr=train_epoch(model,dl_tr,opt,crit,dev,blank)\n",
    "        vl=val_epoch(model,dl_te,crit,dev,blank)\n",
    "        print(f\"Epoch {ep} TL:{tr:.3f} VL:{vl:.3f}\")\n",
    "        if vl<best: best=vl;torch.save(model.state_dict(),'best.pth')\n",
    "\n",
    "    # Load & base eval\n",
    "    model.load_state_dict(torch.load('best.pth'))\n",
    "    preds,refs=evaluate(model,dl_te,proc,dev,blank)\n",
    "    print('Base WER,CER:',wer(preds,refs),cer(preds,refs))\n",
    "\n",
    "    # LLM refine one by one\n",
    "    llm_nm='mistralai/Mistral-7B-Instruct-v0.3'\n",
    "    tok=AutoTokenizer.from_pretrained(llm_nm,token=HF_TOKEN,trust_remote_code=True)\n",
    "    mlm=AutoModelForCausalLM.from_pretrained(llm_nm,device_map='auto',torch_dtype=torch.float16,token=HF_TOKEN,trust_remote_code=True)\n",
    "    gen=GenerationConfig(max_new_tokens=64,do_sample=False)\n",
    "\n",
    "    print('Starting LLM refinement...')\n",
    "    refined=[]\n",
    "    for utt in preds:\n",
    "        if len(utt.split())<6: refined.append(utt)\n",
    "        else: refined.append(refine_one(utt,tok,mlm,gen,dev))\n",
    "\n",
    "    # Clean tags & eval on actual text\n",
    "    def clean(t): return ' '.join(w for w in t.split() if w not in {'EN','HI','|'})\n",
    "    clean_ref=[clean(t) for t in refined]\n",
    "    print('LLM WER,CER:',wer(clean_ref,refs),cer(clean_ref,refs))\n",
    "\n",
    "    # Samples\n",
    "    for i in range(5): print(f\"Sample {i+1}: ASR={preds[i]}\\nRefined={clean_ref[i]}\\nRef={refs[i]}\\n\")\n",
    "\n",
    "if __name__=='__main__': main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0005939-9825-4777-8b9b-107940c47808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
